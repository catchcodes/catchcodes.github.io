<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>CatchCodes</title>
  
  
  <link href="https://catchcodes.github.io/atom.xml" rel="self"/>
  
  <link href="https://catchcodes.github.io/"/>
  <updated>2023-04-14T13:04:40.650Z</updated>
  <id>https://catchcodes.github.io/</id>
  
  <author>
    <name>catchcodes</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>深度学习基础-模型压缩</title>
    <link href="https://catchcodes.github.io/Articles/3213702972.html"/>
    <id>https://catchcodes.github.io/Articles/3213702972.html</id>
    <published>2023-04-14T12:57:38.000Z</published>
    <updated>2023-04-14T13:04:40.650Z</updated>
    
    <content type="html"><![CDATA[<img title="" src="https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/50.jpg" alt="" data-align="center" width="552"><span id="more"></span><h3 id="减少模型参数">减少模型参数</h3><p><strong>网络裁剪</strong><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>network pruning<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span></p><p>通过判断某参数的 l1 或者 l2 范数是否接近于 0 来决定是否要裁剪该参数<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>通常使用 0 来代替被裁剪的权重<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>；</h-inner></h-char></span></p><p>神经元的输出在大多数情况下都为 0 或者接近 0<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>那么我们就可以把这个神经元给裁减掉<span class="bd-box"><h-char class="bd bd-beg"><h-inner>；</h-inner></h-char></span></p><p>裁剪后进行<strong>微调</strong>(fine-tune)填补丢失的性能</p><img title="" src="https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/c05ad483120f4f5ea573f98a09b26c18.png" alt="" data-align="center" width="256"><p><strong>知识蒸馏</strong><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>knowledge distillation<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span></p><p>先训练一个大模型<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>teacher<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>然后再训练一个小模型<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>student<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>来拟合大模型的输出<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这样<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>最终得到的小模型体积比大模型小<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但能获得和大模型接近的性能</p><img src="https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/20230414210326.png" title="" alt="" data-align="center"><p><strong>参数量化</strong><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>parameter quantization<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span></p><p>参数聚类<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>parameter clustering<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>使用聚类算法<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>例如 k-means<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>将相似的参数转为同一个值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>然后记录某个参数属于哪个簇以及该簇对应的参数值是多少即可</p><p><strong>模型结构设计</strong><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>architecture design<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span></p><p><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>比如1x1卷积核<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span> Depthwise Separable Convolution<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>深度可分离卷积<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>,减少卷积核参数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>    假设卷积核输入端的特征图大小为(4x4)x64<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>输出端的大小为(3x3)x128<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>卷积核尺寸为(2x2)<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>单位步长和无填充<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>则深度可分卷积首先使用64个(2x2)x1的卷积核在每个通道分别卷积<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>并组合得到(3x3)x64的张量<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>随后使用128个(1x1)x64的单位卷积核输出结果<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>所需的参数总量为8448<span class="bd-box"><h-char class="bd bd-beg"><h-inner>；</h-inner></h-char></span>若使用标准卷积<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>则128个(2x2)x64的卷积核所需的参数总量为32768<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>是前者的4倍左右</p><h3 id="减少存储空间">减少存储空间</h3><p>16位模型<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span> torch.nn.Module.half()方法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><a href="https://github.com/NVIDIA/apex#quick-start">APEX</a> <span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>深度学习加速库<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>由英伟达开源<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>完美支持PyTorch框架<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>用于改变数据格式来减小模型显存占用的工具<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>其中最有价值的是 amp (Automatic Mixed Precision<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>自动混合精度) <span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>将模型的大部分操作都用 Float16 数据类型测试<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>一些特别操作仍然使用 Float32<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>]]></content>
    
    
    <summary type="html">&lt;img title=&quot;&quot; src=&quot;https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/50.jpg&quot; alt=&quot;&quot; data-align=&quot;center&quot; width=&quot;552&quot;&gt;</summary>
    
    
    
    
    <category term="Deep-Learing" scheme="https://catchcodes.github.io/tags/Deep-Learing/"/>
    
    <category term="深度学习" scheme="https://catchcodes.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="模型压缩" scheme="https://catchcodes.github.io/tags/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9/"/>
    
  </entry>
  
  <entry>
    <title>深度学习基础-激活函数</title>
    <link href="https://catchcodes.github.io/Articles/2553868675.html"/>
    <id>https://catchcodes.github.io/Articles/2553868675.html</id>
    <published>2023-04-14T06:49:22.000Z</published>
    <updated>2023-04-14T12:55:42.062Z</updated>
    
    <content type="html"><![CDATA[<img title="" src="https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/24.jpg" alt="" data-align="center" width="560"><span id="more"></span><h2 id="前言">前言</h2><p>经过上一篇<a href="https://catchcodes.github.io/Articles/2750935351.html">深度学习基础-神经网络</a><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们对神经网络有了一个初步的了解<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>本文将深入讲解激活函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><img title="" src="https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/db92271e3c424f76cac6782410782919.png" alt="" data-align="center" width="605">]]></content>
    
    
    <summary type="html">&lt;img title=&quot;&quot; src=&quot;https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/24.jpg&quot; alt=&quot;&quot; data-align=&quot;center&quot; width=&quot;560&quot;&gt;</summary>
    
    
    
    
    <category term="Deep-Learing" scheme="https://catchcodes.github.io/tags/Deep-Learing/"/>
    
    <category term="深度学习" scheme="https://catchcodes.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="神经网络" scheme="https://catchcodes.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    <category term="激活函数" scheme="https://catchcodes.github.io/tags/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习基础-损失函数</title>
    <link href="https://catchcodes.github.io/Articles/3755838363.html"/>
    <id>https://catchcodes.github.io/Articles/3755838363.html</id>
    <published>2023-04-14T06:48:47.000Z</published>
    <updated>2023-04-14T12:55:31.752Z</updated>
    
    <content type="html"><![CDATA[<img title="" src="https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/33.jpg" alt="" data-align="center" width="533"><span id="more"></span><h2 id="前言">前言</h2><p>经过上一篇<a href="https://catchcodes.github.io/Articles/2750935351.html">深度学习基础-神经网络</a><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们对神经网络有了一个初步的了解<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>本文将深入讲解损失函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><h2 id="选择不同损失函数的原因">选择不同损失函数的原因</h2><p>上文说到<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>损失函数是衡量模型预测值和实际值误差的函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但不同的模型需要不同的指标去衡量误差<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>原因如下<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>1<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>离群点对不同的模型的影响不同<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>如果离群点是需要重点关注的异常值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们要选择均方误差这种对离群点敏感的损失函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>2<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>回归问题和分类问题需要不同的损失函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>前者输出预测值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>更多基于距离度量<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>后者输出预测类别<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>更多基于概率分布度量<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>3<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>不同损失函数的收敛速度不一样<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>要根据模型去选择<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>4<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>不同模型对预测结果的大小应有不同的损失<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>预测错误的梯度幅值应根据模型选择<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><h2 id="损失函数分类">损失函数分类</h2><h4 id="回归问题">回归问题</h4><p>1<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span><strong>绝对值误差MAE</strong><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>L1损失<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span></p><p><span class="markdown-them-math-inline">$ L_{\!M\!A\!E\!}(y,f(x))=|y-f(x)|$</span></p><p>对异常点有更好的鲁棒性<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但对所有点的梯度都一样<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>在0点不可导<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>在0的去心邻域内梯度也很大<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>不利用收敛<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><img title="" src="https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/20230414160725.png" alt="" data-align="center" width="409"><p>2<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span><strong>均方误差</strong><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>L2损失<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span></p><p><span class="markdown-them-math-inline">$ L_{\!M\!S\!E\!}(y,f(x))=[y-f(x)]^2$</span></p><p>计算简便<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>损失的梯度随损失增大而增大<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>而损失趋于0时则会减小<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>对异常点灵敏<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>可能出现梯度爆炸问题<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><img title="" src="https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/20230414160651.png" alt="" data-align="center" width="388"><p>注<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>L1<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>L2是指L1<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>L2范数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>假设<span class="markdown-them-math-inline">$X=(x_1,x_2,\cdots,x_n)$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>则</p><p>L0范数<span class="markdown-them-math-inline">$\| X\|_0$</span>为向量<span class="markdown-them-math-inline">$X$</span>中的非零元素个数</p><p>L1范数为<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><span class="markdown-them-math-inline">$\|X \|_1=\sum \vert x_i\vert$</span></p><p>L2范数为<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><span class="markdown-them-math-inline">$\| X\|_2=\sqrt{\sum x_i^2}$</span></p><p>Lp范数为<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><span class="markdown-them-math-inline">$\| X\|_p=\sqrt[p]{\sum x_i^p}$</span></p><p>p趋于<span class="markdown-them-math-inline">$\infty$</span>时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><span class="markdown-them-math-inline">$\| X\|_{\infty}=max{\vert x_i\vert}$</span></p><p>3<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span><strong>Huber误差</strong></p><p><span class="markdown-them-math-inline">$\displaystyle L_{\delta}(y,f(x))= \begin{cases} \frac{1}{2}(y-f(x))^2 &amp;  |y-f(x)|\leq \delta, \\ \delta|y-f(x)|-\frac{1}{2}\delta^2 &amp; otherwise. \end{cases}$</span></p><p>结合了MAE和MSE的优点 <span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><span class="markdown-them-math-inline">$\delta$</span>为超参数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>需要去评估<span class="markdown-them-math-inline">$\delta$</span>的值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><p>当  <span class="markdown-them-math-inline">$\vert y-f(x) \vert&gt; \delta$</span> 时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>梯度一直近似为 <span class="markdown-them-math-inline">$\delta$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>能够保证模型以一个较快的速度更新参数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>又对离群点较为鲁棒<span class="bd-box"><h-char class="bd bd-beg"><h-inner>；</h-inner></h-char></span><br>当 <span class="markdown-them-math-inline">$\vert y-f(x) \vert\leq\delta$</span> 时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>梯度逐渐减小<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>能够保证模型收敛<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><img title="" src="https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/e79721d83a8671ca860cf4fcbc010993.png" alt="" data-align="center" width="399"><p>4<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span><strong>双曲余弦误差Log-cosh</strong></p><p>二阶处处可微<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><span class="markdown-them-math-inline">$ L(y,f(x))=log[cosh(f(x)-y)]$</span></p><p>相当于处处二次可导的Huber损失<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>用牛顿法优化迭代时就需要二阶导数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>而且没有超参数</p><img title="" src="https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/a7c62f9b24fce5a9dc5a4ff43bc0a561.png" alt="" data-align="center" width="375"><p>5<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span><strong>分位数损失</strong></p><p><span class="markdown-them-math-inline">$\displaystyle L_{\gamma}(y,f(x))=\sum_{i:y_i&lt;f(x_i)}(1-\gamma)|y_i-f(x_i)|+\sum_{i:y_i \geq f(x_i)}\gamma|y_i-f(x_i)|$</span></p><p>通过分位值<span class="markdown-them-math-inline">$ \gamma$</span>对高估和低估给予不同的惩罚<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>反映对正误差和负误差的重视程度<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span></p><p><span class="markdown-them-math-inline">$\gamma=0.5$</span>时相当于绝对值损失<span class="bd-box"><h-char class="bd bd-beg"><h-inner>；</h-inner></h-char></span><span class="markdown-them-math-inline">$\gamma&gt;0.5$</span>时对负误差更敏感<span class="bd-box"><h-char class="bd bd-beg"><h-inner>；</h-inner></h-char></span><span class="markdown-them-math-inline">$\gamma&lt;0.5$</span>时对正误差更敏感<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><img title="" src="https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/0d3b50f9bbb641a7cf92db47558ad089.png" alt="" data-align="center" width="343"><p><span class="markdown-them-math-inline">$\gamma$</span>取不同值可以获取不同分位<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>可以预测结果的上下界</p><img title="" src="https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/20230414183324.png" alt="" width="328" data-align="center"><h4 id="分类问题">分类问题</h4><p>对于分类问题<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>上述损失函数不太适用<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><p>1<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span><strong>Hinge损失</strong></p><p><span class="markdown-them-math-inline">$ L=max\{0\ , 1-yf(x)\}$</span></p><p>适用于<a href="https://zh.wikipedia.org/zh-cn/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA">支持向量机SVM</a>的二分类<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>不仅要分类正确<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>还要让间隔最大<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><img title="" src="https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/v2-3c6aa9626ee8e4609b0d7c5712baf624_r.png" alt="" width="449" data-align="center"><p>下面是其变体<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p><p>原型<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>蓝色<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>在1处不可导</p><p>平方变体<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>绿色<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span><span class="markdown-them-math-inline">$max\{0, 1-yf(x)\}^2$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span></p><p>以及 Rennie 和 Srebro 提出的分段平滑变体<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>红色<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span><span class="markdown-them-math-inline">$\begin{cases}0.5-yf(x) &amp; yf(x)\leq0 \\ 0.5(1-yf(x))^2 &amp; 0&lt;yf(x)&lt;1 \\ 0 &amp; yf(x)\geq 1 \end{cases}$</span></p><img title="" src="https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/Hinge_loss_variants.svg" alt="" width="395" data-align="center"><p>2<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span><strong>交叉熵</strong>损失</p><ul><li><p>二分类用Binary Cross-Entropy <span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p><p><span class="markdown-them-math-inline">$\displaystyle L(y,f(x))=-y\log p-(1-y)\log(1-p)=\begin{cases}-\log p &amp; y=1 \\ -\log(1-p) &amp; y=0 \end{cases}$</span></p></li></ul><p>配上Sigmoid激活函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>只输出一个概率值p</p><ul><li><p>多分类时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>一般都用交叉熵损失函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p><p><span class="markdown-them-math-inline">$\displaystyle L(y,f(x))=-\sum_{c=1}^N y\log(p_c)$</span></p><p>配上softmax激活函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>输出属于各个类别的概率<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><span class="markdown-them-math-inline">$p_c$</span>为预测为类别c的概率<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>N为类别数量</p></li></ul><p>3<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span><strong>focal损失</strong></p><p>预测错误的梯度更大<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>预测正确的梯度小<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>自适应样本的分类难易程度<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><p>其实就是让保持分类正确<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>加大调整分类错误的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><p><span class="markdown-them-math-inline">$\displaystyle L(y,f(x))=\begin{cases}-(1-p)^{\gamma}\log p &amp; y=1 \\ -p^{\gamma}\log(1-p) &amp; y=0 \end{cases}$</span></p><p>蓝线为二分类交叉熵损失</p><img title="" src="https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/20230414200658.png" alt="" data-align="center" width="499">]]></content>
    
    
    <summary type="html">&lt;img title=&quot;&quot; src=&quot;https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/33.jpg&quot; alt=&quot;&quot; data-align=&quot;center&quot; width=&quot;533&quot;&gt;</summary>
    
    
    
    
    <category term="Deep-Learing" scheme="https://catchcodes.github.io/tags/Deep-Learing/"/>
    
    <category term="深度学习" scheme="https://catchcodes.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="神经网络" scheme="https://catchcodes.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    <category term="损失函数" scheme="https://catchcodes.github.io/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习基础-神经网络</title>
    <link href="https://catchcodes.github.io/Articles/2750935351.html"/>
    <id>https://catchcodes.github.io/Articles/2750935351.html</id>
    <published>2023-04-09T09:13:34.000Z</published>
    <updated>2023-04-14T07:15:20.168Z</updated>
    
    <content type="html"><![CDATA[<img title="" src="https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/26.jpg" alt="" data-align="center" width="449"><span id="more"></span><h2 id="前言">前言</h2><p>本文将从单层感知机开始逐步深入到多层感知机<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>解释激活函数的出现和特性<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>解释损失函数的出现<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>解释模型的最终收敛<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>解释参数的初始化准则<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>并有后向传播的公式推导<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><h2 id="神经元">神经元</h2><img title="" src="https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/%E7%A5%9E%E7%BB%8F%E5%85%83.png" alt="" data-align="center" width="415"><p>神经元通过突触将神经递质传递至后一个神经元的树突<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>多个树突接收的信号共同形成细胞整体的电信号并达到兴奋临界点时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>激发电信号<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这就是神经元简化后的工作流程<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><p>下图是神经元的电路模型<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p><img title="" src="https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/209cdf5b0e2830499aad5550ab0690ea.png" alt="" data-align="center" width="405"><h2 id="感知机">感知机</h2><p>弗兰克·罗森布拉特在1957年就职于康奈尔航空实验室<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>Cornell Aeronautical Laboratory<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>时将神经元抽象成<strong>感知机</strong><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>也称为前馈神经网络<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>感知机有n个输入<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>一个输出<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>0或1<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>结构如下图</p><img title="" src="https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/Ncell.png" alt="" data-align="center" width="234"><p><span class="markdown-them-math-inline">$\displaystyle f(x) = \sum_{i=1}^n \omega_i x_i+b\ \ \qquad \qquad  output = \begin{cases} 1 &amp; if\ f(x)&gt;0  \\ 0 &amp; else \end{cases} $</span></p><p>输出<span class="markdown-them-math-inline">$output$</span>是<span class="markdown-them-math-inline">$sgn(f(x)) $</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>符号函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>模拟神经元的激活<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们称之为<strong>激活函数</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这也是感知机与线性回归<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>没有激活函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>和逻辑回归<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>激活函数为sigmoid函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>的区别所在<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><p>注<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>sgn函数是最早的激活函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>现在基本不用了<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><p>感知机是一个最简单的二元分类器<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>也是一个单层的神经网络<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><p>下面我们假设<span class="markdown-them-math-inline">$b=0$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><span class="markdown-them-math-inline">$b\neq0$</span>时让<span class="markdown-them-math-inline">$\omega,x$</span>维数多一维<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><span class="markdown-them-math-inline">$\omega_{n+1}=b \quad x_{n+1}=1$</span>即可</p><p>假设我们有m对数据<span class="markdown-them-math-inline">$D_m = \{(x_1, y_1),\cdots,(x_m, y_m)\}$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><span class="markdown-them-math-inline">$x$</span>为n维向量<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>y是x向量的标签<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>实际模型的输出<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>正确结果<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><p>注<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>输入数据都有标签的这种模式称为<strong>监督学习</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>当输入数据没有标签称为<strong>无监督学习</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>当输入数据只有部分有标签称为<strong>半监督学习</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>当前大部分模型都是监督学习<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><p>感知机通过对<span class="markdown-them-math-inline">$D_m$</span><strong>多次迭代</strong>确定<span class="markdown-them-math-inline">$\omega_i (i=1,\cdots,n)$</span>的最终值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>对于<span class="markdown-them-math-inline">$D_m$</span>中的每对<span class="markdown-them-math-inline">$(x,y)$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><span class="markdown-them-math-inline">$\omega_j := \omega_j + \alpha (y-f(x))x(j) \qquad j=1,\cdots,n$</span></p><p>注意<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>仅当针对给定训练数据<span class="markdown-them-math-inline">$(x,y)$</span>产生的输出值<span class="markdown-them-math-inline">$f(x)$</span>与预期的输出值<span class="markdown-them-math-inline">$y$</span><strong>不同</strong>时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>权重向量<span class="markdown-them-math-inline">$\omega$</span>才会发生改变<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>人类的学习也正是由于这种非预期结果<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><p>至此<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>训练好<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>参数确定好<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>的感知机就可以用于分类了<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><p>但是<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>只有在<span class="markdown-them-math-inline">$D_m$</span>是<strong>线性分隔</strong>时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>才可以在有限次迭代后收敛<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span><span class="markdown-them-math-inline">$\omega_j$</span>趋于特定值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><blockquote><p>如果存在一个正的常数<span class="markdown-them-math-inline">$\gamma$</span>和权重向量<span class="markdown-them-math-inline">$\omega$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>对所有的<span class="markdown-them-math-inline">$i$</span>满足<span class="markdown-them-math-inline">$y_i \cdot(\omega\cdot x_i+b)&gt;\gamma$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>训练集<span class="markdown-them-math-inline">$D_m$</span>就被叫被做线性分隔的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>同时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>假设<span class="markdown-them-math-inline">$\displaystyle R=\max_{1\leq i \leq m}\|x_i\|$</span> <span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>则错误次数<span class="markdown-them-math-inline">$k \leq (\frac{R}{\gamma})^2$</span></p></blockquote><p>所以<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这种单层的感知机只能做简单的线性分类任务<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>异或都无法实现<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>而如果将计算层增加到两层<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>计算量则过大<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>1970年左右<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>而且没有有效的学习算法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>其实<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>1974年哈佛大学的Paul Werbos就证明了增加一个网络层<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>利用反向传播算法可以搞定XOR问题<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但却未得到重视<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><h2 id="多层感知机">多层感知机</h2><p>1986年<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>Rumelhar和<strong>Hinton</strong><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>深度学习鼻祖<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>等人提出了反向传播<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>Backpropagation<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>BP<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>算法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>解决了两层神经网络所需要的复杂计算量问题<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>并广泛应用于升级网络的训练中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><p>下图是一个简单的3输入<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>2输出的双层感知机<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>前馈神经网络<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>中间为4个节点的隐藏层<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><p>一般来说<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>输入节点数量和输出节点数量都是确定的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>隐藏层节点数量需要自己设计<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>关键点<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><blockquote><p>StackOverflow上有一个经验公式<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><span class="markdown-them-math-inline">$N_h = \frac{N_s}{\alpha (N_i+N_o)}$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><span class="markdown-them-math-inline">$N_s$</span>为训练集样本数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><span class="markdown-them-math-inline">$\alpha$</span>一般取2~10<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><span class="markdown-them-math-inline">$N_i$</span>为输入层节点数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><span class="markdown-them-math-inline">$N_o$</span>为输出层节点数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>在此经验公式上再试验调整即可<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p></blockquote><img title="" src="https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/20230409202342.png" alt="" width="346" data-align="center"><p><span class="markdown-them-math-inline">$a(\cdot)$</span>为激活函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>如单层感知机的sgn函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但在多层感知机中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>由于涉及到梯度的计算<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>一般用Sigmoid<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>ReLU<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>tanh等激活函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><p><strong>说明</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><span class="markdown-them-math-inline">$w^{(1)}$</span>指第一层权重<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><span class="markdown-them-math-inline">$w_1^{(1)}$</span>指<span class="markdown-them-math-inline">$x_1$</span> 对应的第一层权重<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><span class="markdown-them-math-inline">$w_1^{(1)}[1]$</span>指<span class="markdown-them-math-inline">$x_1$</span>对应的第一层的权重的第一个值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><p><span class="markdown-them-math-inline">$h_1 = a(\sum_{i=1}^3 w^{(1)}_i[1] x_i) \quad$</span> <span class="markdown-them-math-inline">$h_2 = a(\sum_{i=1}^3 w^{(1)}_i[2]x_i) \quad$</span> <span class="markdown-them-math-inline">$h_3 = a(\sum_{i=1}^3 w^{(1)}_i[3]x_i) \quad$</span> <span class="markdown-them-math-inline">$h_4 = a(\sum_{i=1}^3 w^{(1)}_i[4]x_i)$</span></p></br><p><span class="markdown-them-math-inline">$z_1 = \sum_{j=1}^4 w^{(2)}_j[1] h_j \qquad$</span> <span class="markdown-them-math-inline">$ z_2=\sum_{j=1}^4 w^{(2)}_j[2]h_j$</span></p></br><p><span class="markdown-them-math-inline">$ y_1 = a(z_1)=f(x)[1] \qquad $</span> <span class="markdown-them-math-inline">$y_2 = a(z_2)=f(x)[2]$</span></p><p>其中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><span class="markdown-them-math-inline">$f(x)=(a(z_1),a(z_2))$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>后面为索引<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><p>对于偏置节点<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>只需让<span class="markdown-them-math-inline">$x_4=1,w^{(1)}_5=b^{(1)} \quad$</span> <span class="markdown-them-math-inline">$ h_5=1,w^{(2)}_3=b^{(2)}$</span> <span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>故先不考虑<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><p>当中间隐藏层的神经元数量趋于无穷多时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>多层感知机可以拟合任何非线性函数<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>由于激活函数的存在<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>若没有激活函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>多层线性层也只相当于一层线性层<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>可见<a href="https://zhuanlan.zhihu.com/p/458006332">中文版证明过程</a>或<a href="https://www.zhihu.com/question/268384579/answer/540793202">直观理解</a><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>因此可以解决非线性分类任务<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><h4 id="后向传播bp算法">后向传播BP算法</h4><p>训练模型的目的是去拟合实际情况<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>即让训练模型参数<span class="markdown-them-math-inline">$w$</span>和实际模型的参数一致<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>因为只能获取真实情况的标签<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>一个朴素的想法就是让模型的输出与实际接近<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>那<strong>参数分布</strong>基本就与实际相似了<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><p>那么如何衡量这个误差呢<span class="bd-box"><h-char class="bd bd-beg"><h-inner>？</h-inner></h-char></span>于是定义了一个指标去衡量误差——<strong>损失函数</strong>(Loss-Function)<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>那么我们的目的就是让<strong>所有样本的损失函数</strong>最小<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><p>注<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>常见的损失函数有绝对值损失<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>均方损失<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>Huber损失<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>交叉熵损失函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>实际评判的标准还是在另一批数据<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span><strong>测试集</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>的表现<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>损失函数只能判断是否收敛<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><p>我们称之为<strong>期望(expected)风险</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><strong>所有</strong>样本的平均误差<span class="markdown-them-math-inline">$ R_{exp}(f)=E_p[L(Y,f(X))]$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><span class="markdown-them-math-inline">$Y$</span>为标签集合<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><span class="markdown-them-math-inline">$X$</span>为样本集合<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><span class="markdown-them-math-inline">$L$</span>为损失函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>为全局最优<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><p>进一步的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><span class="markdown-them-math-inline">$\displaystyle R_{exp}(f)= \int_{X\times Y } L(y,f(x))P(x,y)dxdy \quad$</span> <span class="markdown-them-math-inline">$ P(x,y)$</span>为真实分布</p><p>但是我们手中只有部分的数据<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>训练数据<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>不可能获取到所有的数据<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>更不用说真实分布<span class="markdown-them-math-inline">$P(X,Y)$</span>我们都无从得知<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>于是我们用我们<strong>训练集的数据去估计整体的误差</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><p>我们称之为<strong>经验(empirical)风险</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>训练集上的平均误差<span class="markdown-them-math-inline">$ R_{emp}(f)=\frac{1}{N}\sum^N_{i=1}L(y_i,f(x_i))$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>为局部最优<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>当数据量足够大时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>经验风险就趋于期望风险<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><hr><p>注<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>还有<strong>结构(structural)风险</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p><p>为了让权值不会过大(参数稀疏化)并减轻过拟合程度<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>让参数变为0或接近0<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>边界较为平滑<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>加上<strong>正则项</strong><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>也叫权重衰减项weight decay term<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>惩罚项<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span></p><blockquote><p>奥卡姆剃刀原则<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>如无必要<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>勿增实体<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p></blockquote><p><span class="markdown-them-math-inline">$ R_{str}(f)=\frac{1}{N}\sum^N_{i=1}L(y_i,f(x_i))+\lambda J(f)$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><span class="markdown-them-math-inline">$\lambda$</span>称为惩罚因子或weight decay parameter</p><img title="" src="https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/898772cc648809a38dd6bff6e0bf7f96.png" alt="" data-align="center" width="470"><hr><p>所以<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们现在的问题是如何使损失函数<span class="markdown-them-math-inline">$L$</span>最小<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这正是一个<strong>最优化问题</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们很容易想到令损失函数对参数的偏导为0<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>再从这些点中找出最优点<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>但是<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>第一<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>对于非凸函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>偏导为0并不等价于极值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>；</h-inner></h-char></span>第二<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>偏导为0的点可能有无穷多个<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>无法找出最优点<span class="bd-box"><h-char class="bd bd-beg"><h-inner>；</h-inner></h-char></span>第三<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>求解偏导为0即对求解一个非线性方程组<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>激活函数的存在<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>可能根本求不出<strong>解析解</strong><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>直接求解需要求矩阵的逆<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>并非所有矩阵都可逆<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>而且计算量较大<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><p>所以我们只能去求<strong>数值解</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>一个普遍的算法<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>或者<a href="https://zh.wikipedia.org/zh-cn/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95">遗传算法</a><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>也有用<a href="https://arxiv.org/abs/1504.07278">HJB方程更新参数</a>的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>是使用<strong>梯度下降(GD)算法</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>即对训练集<span class="markdown-them-math-inline">$D_m$</span>中<strong>所有</strong>的样本<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>m组数据<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>迭代</p><p><span class="markdown-them-math-inline">$\displaystyle w_j^1[t]=w_j^1[t] - \eta_1\frac{1}{m} \sum_{i=1}^m \frac{\partial L(y_i^*,f(x_i))}{\partial w_j^1[t]} \quad t=1,2,3,4$</span></p><p><span class="markdown-them-math-inline">$\displaystyle w_j^{(2)}[t]=w_j^{(2)}[t] - \eta_2\frac{1}{m} \sum_{i=1}^m \frac{\partial L(y_i^*,f(x_i))}{\partial w_j^{(2)}[t]} \quad t=1,2$</span></p><p><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span><span class="markdown-them-math-inline">$y^*$</span>为真实标签向量<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><span class="markdown-them-math-inline">$w$</span>和<span class="markdown-them-math-inline">$x_i$</span>为向量<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但<span class="markdown-them-math-inline">$w[t]$</span>为标量<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><span class="markdown-them-math-inline">$\eta_1 \ \eta_2$</span> 为步长<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>也称为<strong>学习率</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>直至收敛<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>因为梯度的反方向是下降最快的方向即收敛最快的方向<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><p>学习率不宜过大<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>导致模型不能收敛或在最优点徘徊<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>或过小<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>收敛很慢<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>更容易进入局部最优点<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>局部极小值点<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>或鞍点<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>更多的是鞍点<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>可见[论文](1605.07110] Deep Learning without Poor Local Minima (<a href="http://arxiv.org">arxiv.org</a>)](<a href="https://arxiv.org/abs/1605.07110">https://arxiv.org/abs/1605.07110</a>)<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>通常取值为0.01-0.001<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>一般来说每一层的学习率都是一样的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但是可以根据自己需求去设置每一层的学习率<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>学习率随训练轮数的增加可以适当降低<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><img title="" src="https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/20230411142400.png" alt="" width="429" data-align="center"><p>注<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>鞍点的Hessian矩阵不定<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>特征值有正有负<span class="bd-box"><h-char class="bd bd-beg"><h-inner>；</h-inner></h-char></span>极小值点的Hessian矩阵正定<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>特征值都为正<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><p>有<a href="https://dl.acm.org/doi/10.5555/3305890.3305970">论文</a>说明<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p><img title="" src="https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/20230411184308.png" alt="" width="558" data-align="center"><p><span class="markdown-them-math-inline">$\phi$</span>为参数与数据量的比值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><span class="markdown-them-math-inline">$\epsilon$</span>为损失值Loss</p><ul><li>当Loss很大的时候<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>特征值分布有正有负<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><strong>表明鞍点是困扰优化的主要原因</strong></li><li>当Loss很小的时候<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>鞍点逐渐消失<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><strong>主要是局部极小值点</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></li></ul><p>大多数情况下<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>局部最优和全局最优几乎一样好<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>局部最优已经能满足我们的需要<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>所以重点是去避免陷入鞍点和泛化不好的局部最优点<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>而下文的SGD可以逃离鞍点和泛化不好的局部最优点<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>主要是sharp minima<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>更倾向于收敛到flat minima<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><hr><p>那么<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们的重点便是计算<span class="markdown-them-math-inline">$\frac{\partial L}{\partial w}$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>以上面的多层感知机为例子<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p><p>假设其损失函数为均方损失<span class="markdown-them-math-inline">$\displaystyle L(y^*,y)=\frac{1}{k} \sum_{i=1}^k(y_i-y^*_i)^2 \quad$</span> <span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>本例中输出节点数量<span class="markdown-them-math-inline">$k=2$</span></p><p>又由上文可知<span class="markdown-them-math-inline">$\displaystyle y_i = a(z_i) \  =  a(\sum_{j=1}^4 w^{(2)}_j[i] h_j)=a(\sum_{j=1}^4 w^{(2)}_j[i] a(\sum_{n=1}^3w_n^{(1)}[j]x_n))$</span></p><p>所以<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>对第二层的梯度计算如下<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>根据链式法则<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p><p><span class="markdown-them-math-inline">$\displaystyle \frac{\partial L(y^*,y)}{\partial w^{(2)}_l[t]}=\frac{2}{k}\cdot (y_t-y^*_t) \frac{\partial y_t}{\partial w^{(2)}_l[t]}=\frac{2}{k}\cdot (y_t-y^*_t)\cdot \frac{\partial a(z_t)}{\partial z_t} \cdot \frac{\partial z_t}{\partial w^{(2)}_l[t]}=\frac{2}{k}\cdot (y_t-y^*_t)\cdot \frac{\partial a(z_t)}{\partial z_t} \cdot h_l$</span></p><p>其中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><span class="markdown-them-math-inline">$\frac{\partial a(\cdot)}{\partial \cdot}=a'(\cdot)\quad k=2 \qquad l=1,2,3,4 \quad t=1,2$</span></p><p>选择其他损失函数时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><span class="markdown-them-math-inline">$2(y_t-y^*_t)$</span>换成<span class="markdown-them-math-inline">$\displaystyle \frac{\partial L(y,y^*)}{\partial y_t}$</span>即可</p></br><p>对第一层的梯度计算较为复杂<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>直接计算如下<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p><p><span class="markdown-them-math-inline">$\displaystyle \frac{\partial L(y^*,y)}{\partial w^{(1)}_l[t]}=\frac{2}{k}\cdot  \sum_{i=1}^2 (y_i-y^*_i)\cdot\frac{\partial y_i}{\partial w^{(1)}_l[t]}$</span></p></br><p><span class="markdown-them-math-inline">$\displaystyle=\frac{2}{k}\cdot \sum_{i=1}^2 (y_i-y^*_i)\cdot\frac{\partial a(z_i)}{\partial z_i}\frac{\partial z_i}{\partial w^{(1)}_l[t]}$</span></p></br><p><span class="markdown-them-math-inline">$=\displaystyle \frac{2}{k}\cdot [(y_1-y^*_1)\cdot\frac{\partial a(z_1)}{\partial z_1}\cdot w^{(2)}_t[1]\cdot \frac{\partial h_t}{\partial w^{(1)}_l[t]}+(y_2-y^*_2)\cdot\frac{\partial a(z_2)}{\partial z_2}\cdot w_t^{(2)}[2]\cdot \frac{\partial h_t}{\partial w^{(1)}_l[t]}]$</span></p></br><p><span class="markdown-them-math-inline">$=\displaystyle \frac{2}{k}\cdot[(y_1-y^*_1)\cdot\frac{\partial a(z_1)}{\partial z_1}\cdot w^{(2)}_t[1] +(y_2-y^*_2)\cdot\frac{\partial a(z_2)}{\partial z_2}\cdot w_t^{(2)}[2]]\frac{\partial a(\sum_{i=1}^3 w^{(1)}_i[t] x_i)}{\partial w^{(1)}_l[t]}$</span></p></br><p><span class="markdown-them-math-inline">$=\displaystyle \frac{2}{k}\cdot[(y_1-y^*_1)\cdot\frac{\partial a(z_1)}{\partial z_1}\cdot w^{(2)}_t[1] +(y_2-y^*_2)\cdot\frac{\partial a(z_2)}{\partial z_2}\cdot w_t^{(2)}[2]]\cdot \frac{\partial a(\sum_{i=1}^3 w^{(1)}_i[t] x_i)}{\partial \sum_{i=1}^3 w^{(1)}_i[t] x_i}\cdot x_l $</span></p><p>其中<span class="markdown-them-math-inline">$l=1,2,3 \quad t=1,2,3,4$</span></p><p>但是我们可以借助第二层的梯度计算结果<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>简化为<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p><p><span class="markdown-them-math-inline">$\displaystyle \frac{\partial L(y^*,y)}{\partial w^{(1)}_l[t]}=[\frac{\partial L(y^*,y)}{\partial w_l^{(2)}[1]}\cdot \frac{w_t^{(2)}[1]}{h_l}+\frac{\partial L(y^*,y)}{\partial w_l^{(2)}[2]}\cdot \frac{w_t^{(2)}[2]}{h_l}]\cdot \frac{\partial a(\sum_{i=1}^3 w^{(1)}_i[t] x_i)}{\partial \sum_{i=1}^3 w^{(1)}_i[t] x_i}\cdot x_l $</span></p><p>将训练集中每个样本的梯度计算出来后便更新一次参数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>如此迭代直至收敛<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><p>这可以保证向<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>局部<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>最优点移动<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但是当数据量较大时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>一次迭代较慢<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>训练过程很慢<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>占用内存较大<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>而且进行了很多冗余的计算<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>其他的做法有<strong>随机梯度下降</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span><strong>批量梯度下降</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><p><strong>随机梯度下降Stochastic Gradient Descent</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>每次从训练集随机选一个样本去更新参数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>参数更新速度大大提高<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>降低了计算开销<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>而且有可能收敛到更好的局部最优点<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但是这相当于对梯度的噪声估计<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>参数的变化较为振荡<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>甚至有可能不收敛<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>但有研究显示当我们慢慢的降低学习率时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>SGD 拥有和 GD 一样的收敛性能<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>对于非凸和凸曲面几乎同样能够达到局部或者全局最优点<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>所以基本没人用GD<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><p><strong>批量梯度下降Mini-Batch Gradient Descent</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>每次从训练集选择k个样本去更新参数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><span class="markdown-them-math-inline">$1&lt;k&lt;m$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>是对GD和SGD的折中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>结合了二者的优点<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>也是实际中常用的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>通常我们称<span class="markdown-them-math-inline">$k$</span>为一个mini batch<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>每一轮迭代更新称为一个iteration<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>取完所有样本称为一个epoch<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><hr><h5 id="激活函数">激活函数</h5><p>接下来就是激活函数<span class="markdown-them-math-inline">$a(\cdot)$</span>的选择<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>由上式计算可以看出激活函数的梯度不能为0<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>否则无法更新参数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这就是sgn函数不适用多层感知机的原因<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>此外<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>前向传播的过程中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>每一层的输出都要经过激活函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们希望激活函数能将输出值限制在一个范围内<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这样对于一些较大的输入<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>模型也能保持稳定<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但是这会限制模型的表达能力<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>故这需要根据自己的需求判断<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><p>以下是激活函数应该具有的特征<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p><ul><li>激活函数应该为神经网络引入<strong>非线性</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>即不是y=ax+b这种</li><li>避免<strong>梯度弥散</strong><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>即梯度接近或等于0<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>或<strong>梯度爆炸</strong><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>梯度大于或远大于1<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>的特性<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>造成网络更新过慢<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>由梯度计算式可以看到第一层的梯度有每一层激活函数导数的连乘<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>若是层数加深<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>不合理的激活函数会使梯度容易出现0值或<span class="markdown-them-math-inline">$\infty$</span>值</li><li>输出最好<strong>关于0对称</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>否则这一层输出都为正数或负数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>导致下一层的参数更新都是增加或减少<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>称为zig zag path<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>不利于参数更新</li><li>激活函数应该是<strong>可微的</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>使得满足各层之间梯度下降的计算(至少部分可微)</li><li>梯度的<strong>计算不应该太复杂</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>影响网络性能</li></ul><p>下图是三种常用的激活函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>其他函数可见<a href="https://en.wikipedia.org/wiki/Activation_function">维基百科</a></p><img title="" src="https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/20230411120953.png" alt="" data-align="center" width="579"><hr><h5 id="参数初始化">参数初始化</h5><p>通过梯度的计算式<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们不难发现参数的更新与其<strong>初始值</strong>有关<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>其收敛速度跟初始值有很大关系<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>若初始值都一样<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>那么参数的更新也会一样<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>整个网络的参数都会一样<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这显然不符合我们的预期<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>所以<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><strong>参数的初始化</strong>必须是<strong>随机</strong>的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>不宜过大也不宜过小<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>否则会出现梯度消失或梯度爆炸<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>故我们需要控制初值的方差<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>最好保证输入输出的<strong>方差一致</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>而且其均值应为0<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>全为正或全为负会出现上文的zig zag path现象<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>一个好的参数初始化虽然不能完全解决<strong>梯度消失</strong>或<strong>梯度爆炸</strong>的问题<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>BatchNorm解决了<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但是对于处理这两个问题是有很大帮助的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>并且十分有利于提升模型的收敛速度和性能表现<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><p>下图是一些初始化的方法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>一个直观的解释可见<a href="https://spaces.ac.cn/archives/7180">从几何视角来理解模型参数的初始化</a><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>对于tanh和Sigmoid这种<strong>饱和激活函数</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>Xavier初始化较为常用<span class="bd-box"><h-char class="bd bd-beg"><h-inner>；</h-inner></h-char></span>对于ReLU及其变种等<strong>非饱和激活函数</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>He初始化较为常用<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><p>注<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p><ol><li>x 趋于正无穷时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>激活函数的导数趋于 0<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>则我们称之为<strong>右饱和</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>；</h-inner></h-char></span></li><li>x 趋于负无穷时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>激活函数的导数趋于 0<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>则我们称之为<strong>左饱和</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>；</h-inner></h-char></span></li><li>当一个函数既满足右饱和又满足左饱和时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们称之为<strong>饱和激活函数</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></li></ol><img title="" src="https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/df88506793f9ab7378cf838226f7660a.png" alt="" data-align="center" width="582"><p><span class="markdown-them-math-inline">$fan\_in$</span>是指这层的输入节点数量<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><span class="markdown-them-math-inline">$fan\_out$</span>是这层的输出节点数量<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>]]></content>
    
    
    <summary type="html">&lt;img title=&quot;&quot; src=&quot;https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/26.jpg&quot; alt=&quot;&quot; data-align=&quot;center&quot; width=&quot;449&quot;&gt;</summary>
    
    
    
    
    <category term="Deep-Learing" scheme="https://catchcodes.github.io/tags/Deep-Learing/"/>
    
    <category term="深度学习" scheme="https://catchcodes.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="神经网络" scheme="https://catchcodes.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>HSV空间替换纯色衣服颜色</title>
    <link href="https://catchcodes.github.io/Articles/2372941552.html"/>
    <id>https://catchcodes.github.io/Articles/2372941552.html</id>
    <published>2023-04-09T07:28:03.000Z</published>
    <updated>2023-04-16T07:52:20.473Z</updated>
    
    <content type="html"><![CDATA[<img title="" src="https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/31.jpg" alt="" data-align="center" width="477"><span id="more"></span><p>本文将介绍如何在HSV空间替换衣服的颜色<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>仅限纯色衣服<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><p>完整代码可见我的Github<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><a href="https://github.com/catchcodes/ChangeCloth">GitHub - catchcodes/ChangeCloth: 在HSV空间替换衣服颜色</a></p><p>包括单纯opencv和用pyqt5实现图形界面的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><h2 id="hsv详解">HSV详解</h2><h4 id="hsv颜色空间介绍">HSV颜色空间介绍</h4><p>HSV是一种将RGB色彩模型中的点在<strong>圆柱坐标系</strong>中的表示方法</p><ul><li><p><strong>色相</strong><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>Hue<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>是色彩的基本属性<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>用角度度量<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>取值范围为0°～360°<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>在OpenCV中为0-180<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>是由于8bit的最大值为255<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>从红色开始按逆时针方向计算<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>红色为0°<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>绿色为120°,蓝色为240°<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p></br></li><li><p><strong>饱和度</strong><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>Saturation<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>表示颜色接近光谱色的程度<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span><em>一种颜色<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>可以看成是某种光谱色与白色混合的结果</em><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>其中光谱色所占的比例愈大<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>颜色接近光谱色的程度就愈高<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>颜色的饱和度也就愈高<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>饱和度高<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>颜色则深而艳<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>光谱色的白光成分为0<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>饱和度达到最高<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>通常取值范围为0%～100%<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>值越大<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>颜色越饱和<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>在OpenCV中为0-255<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p></br></li><li><p><strong>明度</strong><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>Value<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>明度表示颜色明亮的程度<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>对于光源色<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>明度值与发光体的光亮度有关<span class="bd-box"><h-char class="bd bd-beg"><h-inner>；</h-inner></h-char></span>对于物体色<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>此值和物体的透射比或反射比有关<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>光照对此值影响最大<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>通常取值范围为0%<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>黑<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>到100%<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>在OpenCV中为0-255<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p></li></ul><img src="https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/HSV%E8%A1%A8%E6%A0%BC.png" title="" alt="" data-align="center"><h4 id="rgb转换为hsv">RGB转换为HSV</h4><p><span class="markdown-them-math-inline">$ r = R/255.0, \ g = G/255.0 , \ b=B/255.0 $</span></p><p><span class="markdown-them-math-inline">$ max = \max(r, g, b), \ min = \min(r, g, b) $</span></p><img title="" src="https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/H%E5%88%86%E9%87%8F.svg" alt="" data-align="center" width="391"></br><img title="" src="https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/S%E5%88%86%E9%87%8F.svg" alt="" data-align="center" width="332"></br><img src="https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/V%E5%88%86%E9%87%8F.svg" title="" alt="" data-align="center"><h4 id="hsv与hsl的对比">HSV与HSL的对比</h4><p>注<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>HSL也称HSI<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>I<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>Intensity<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>或HLS</p><p>上一张经典的对比图<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>图源Wikipedia<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span></p><img title="" src="https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/R.png" alt="" width="454" data-align="center"><ul><li><p>在HSL中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>亮度L分量跨越<strong>从黑色经过选择的色相到白色的完整范围</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>；</h-inner></h-char></span></p></li><li><p>在HSV中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>V分量只走一半行程<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><strong>从黑色到选择的色相</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p></li></ul><p>下列左图为HSL<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>右图为HSV</p><img title="" src="https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/HSV%E4%B8%8EHSL%E5%AF%B9%E6%AF%94.png" alt="" data-align="center" width="577"><h2 id="代码实现">代码实现</h2><p><strong>第一步</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>点击图片获取要替换的衣服BGR颜色<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>也可直接根据图片H分量范围去选择<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span></p><div class="highlight"><pre class="code"><code>img = cv2.imread(<span class="hljs-string">r&quot;图片路径&quot;</span>)rows, cols, _ = img_hsv.shapecv2.namedWindow(<span class="hljs-string">&quot;ChangeColor&quot;</span>)cv2.resizeWindow(<span class="hljs-string">&quot;ChangeColor&quot;</span>, rows, cols)<span class="hljs-comment"># 创建一个鼠标事件响应</span>cv2.setMouseCallback(<span class="hljs-string">&quot;ChangeColor&quot;</span>, get_color_BGR)<span class="hljs-comment"># 这是setMouseCallback的声明原型&lt;span class=&quot;bd-box&quot;&gt;&lt;h-char class=&quot;bd bd-beg&quot;&gt;&lt;h-inner&gt;，&lt;/h-inner&gt;&lt;/h-char&gt;&lt;/span&gt;参数分别为窗口名&lt;span class=&quot;bd-box&quot;&gt;&lt;h-char class=&quot;bd bd-beg&quot;&gt;&lt;h-inner&gt;、&lt;/h-inner&gt;&lt;/h-char&gt;&lt;/span&gt;鼠标响应函数&lt;span class=&quot;bd-box&quot;&gt;&lt;h-char class=&quot;bd bd-beg&quot;&gt;&lt;h-inner&gt;、&lt;/h-inner&gt;&lt;/h-char&gt;&lt;/span&gt;传给鼠标响应函数参数</span><span class="hljs-comment"># def setMouseCallback(windowName: Any,</span><span class="hljs-comment">#                      onMouse: Any,</span><span class="hljs-comment">#                      param: Any = None) -&gt; None</span><span class="hljs-comment"># 回调函数&lt;span class=&quot;bd-box&quot;&gt;&lt;h-char class=&quot;bd bd-beg&quot;&gt;&lt;h-inner&gt;，&lt;/h-inner&gt;&lt;/h-char&gt;&lt;/span&gt; 即鼠标事件响应函数</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_color_BGR</span>(<span class="hljs-params">envent, x, y, flags, param</span>):</span>    <span class="hljs-keyword">if</span> envent == cv2.EVENT_LBUTTONDOWN:        <span class="hljs-comment"># 注意&lt;span class=&quot;bd-box&quot;&gt;&lt;h-char class=&quot;bd bd-beg&quot;&gt;&lt;h-inner&gt;：&lt;/h-inner&gt;&lt;/h-char&gt;&lt;/span&gt;opencv的行列与坐标相反(row, col) -&gt; (y, x)</span>        BGR = img[y, x].tolist()     </code></pre></div><p><strong>第二步</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>将获取到的BGR值转为HSV</p><div class="highlight"><pre class="code"><code>HSV = [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">256</span>]  <span class="hljs-comment"># 一个不存在的HSV值</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bgr2hsv</span>(<span class="hljs-params">b, g, r</span>):</span>    <span class="hljs-comment"># 限定像素BGR值的范围</span>    <span class="hljs-keyword">assert</span> (<span class="hljs-number">0</span> &lt;= r &lt; <span class="hljs-number">256</span> <span class="hljs-keyword">and</span> <span class="hljs-number">0</span> &lt;= g &lt; <span class="hljs-number">256</span> <span class="hljs-keyword">and</span> <span class="hljs-number">0</span> &lt;= b &lt; <span class="hljs-number">256</span>)    r, g, b = r / <span class="hljs-number">255.0</span>, g / <span class="hljs-number">255.0</span>, b / <span class="hljs-number">255.0</span>    Max = <span class="hljs-built_in">max</span>(r, g, b)    Min = <span class="hljs-built_in">min</span>(r, g, b)    D = Max - Min    HSV[<span class="hljs-number">2</span>] = <span class="hljs-built_in">int</span>(Max * <span class="hljs-number">255</span>)    HSV[<span class="hljs-number">1</span>] = <span class="hljs-built_in">int</span>(<span class="hljs-number">255</span> * (D / Max)) <span class="hljs-keyword">if</span> Max != <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span>    <span class="hljs-keyword">if</span> D == <span class="hljs-number">0</span>:        HSV[<span class="hljs-number">0</span>] = <span class="hljs-number">0</span>    <span class="hljs-keyword">elif</span> Max == r:        HSV[<span class="hljs-number">0</span>] = (<span class="hljs-number">60</span> * ((g - b) / D)) % <span class="hljs-number">360</span>    <span class="hljs-keyword">elif</span> Max == g:        HSV[<span class="hljs-number">0</span>] = (<span class="hljs-number">120</span> + <span class="hljs-number">60</span> * ((b - r) / D)) % <span class="hljs-number">360</span>    <span class="hljs-keyword">elif</span> Max == b:        HSV[<span class="hljs-number">0</span>] = (<span class="hljs-number">240</span> + <span class="hljs-number">60</span> * ((r - g) / D)) % <span class="hljs-number">360</span>    HSV[<span class="hljs-number">0</span>] = <span class="hljs-built_in">int</span>(HSV[<span class="hljs-number">0</span>] / <span class="hljs-number">2</span>)    <span class="hljs-keyword">return</span> HSV</code></pre></div><p><strong>第三步</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>限定H分量的范围<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>找到衣服的区域</p><div class="highlight"><pre class="code"><code>img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)<span class="hljs-comment"># HSV 的下界限</span>lower = np.array([HSV[<span class="hljs-number">0</span>] - <span class="hljs-number">7</span>, <span class="hljs-number">50</span>, <span class="hljs-number">50</span>])<span class="hljs-comment"># HSV 的上界限</span>upper = np.array([HSV[<span class="hljs-number">0</span>] + <span class="hljs-number">7</span>, <span class="hljs-number">255</span>, <span class="hljs-number">255</span>])<span class="hljs-comment"># 找到H分量在这个范围的区域</span>mask = cv2.inRange(img_hsv, lower, upper)<span class="hljs-comment"># 腐蚀和膨胀的核kernel&lt;span class=&quot;bd-box&quot;&gt;&lt;h-char class=&quot;bd bd-end&quot;&gt;&lt;h-inner&gt;（&lt;/h-inner&gt;&lt;/h-char&gt;&lt;/span&gt;可根据图片自己设计&lt;span class=&quot;bd-box&quot;&gt;&lt;h-char class=&quot;bd bd-beg&quot;&gt;&lt;h-inner&gt;）&lt;/h-inner&gt;&lt;/h-char&gt;&lt;/span&gt;</span>kernel = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]], dtype=np.uint8)<span class="hljs-comment"># 腐蚀图像&lt;span class=&quot;bd-box&quot;&gt;&lt;h-char class=&quot;bd bd-beg&quot;&gt;&lt;h-inner&gt;，&lt;/h-inner&gt;&lt;/h-char&gt;&lt;/span&gt;除去干扰点</span>eroded = cv2.erode(mask, kernel, iterations=<span class="hljs-number">1</span>)<span class="hljs-comment"># 膨胀图像&lt;span class=&quot;bd-box&quot;&gt;&lt;h-char class=&quot;bd bd-beg&quot;&gt;&lt;h-inner&gt;，&lt;/h-inner&gt;&lt;/h-char&gt;&lt;/span&gt;恢复腐蚀前的大小</span>dilated = cv2.dilate(eroded, kernel, iterations=<span class="hljs-number">1</span>)</code></pre></div><p><strong>第四步</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>在窗口中控制H分量和显示图像</p><div class="highlight"><pre class="code"><code><span class="hljs-comment"># 在‘ChangeColor’窗口中创建滑动控制条</span><span class="hljs-comment"># 参数介绍 createTrackbar(const String&amp; trackbarname, const String&amp; winname, int *value, int count, TrackbarCallback onChange = 0, void *userdata = 0)</span><span class="hljs-comment"># 分别为滑动条名称&lt;span class=&quot;bd-box&quot;&gt;&lt;h-char class=&quot;bd bd-beg&quot;&gt;&lt;h-inner&gt;、&lt;/h-inner&gt;&lt;/h-char&gt;&lt;/span&gt;窗口名称&lt;span class=&quot;bd-box&quot;&gt;&lt;h-char class=&quot;bd bd-beg&quot;&gt;&lt;h-inner&gt;、&lt;/h-inner&gt;&lt;/h-char&gt;&lt;/span&gt;初始值&lt;span class=&quot;bd-box&quot;&gt;&lt;h-char class=&quot;bd bd-beg&quot;&gt;&lt;h-inner&gt;，&lt;/h-inner&gt;&lt;/h-char&gt;&lt;/span&gt;范围&lt;span class=&quot;bd-box&quot;&gt;&lt;h-char class=&quot;bd bd-beg&quot;&gt;&lt;h-inner&gt;，&lt;/h-inner&gt;&lt;/h-char&gt;&lt;/span&gt;滑块更改时的回调函数&lt;span class=&quot;bd-box&quot;&gt;&lt;h-char class=&quot;bd bd-beg&quot;&gt;&lt;h-inner&gt;，&lt;/h-inner&gt;&lt;/h-char&gt;&lt;/span&gt;传给回调函数的参数&lt;span class=&quot;bd-box&quot;&gt;&lt;h-char class=&quot;bd bd-end&quot;&gt;&lt;h-inner&gt;（&lt;/h-inner&gt;&lt;/h-char&gt;&lt;/span&gt;默认为0&lt;span class=&quot;bd-box&quot;&gt;&lt;h-char class=&quot;bd bd-beg&quot;&gt;&lt;h-inner&gt;）&lt;/h-inner&gt;&lt;/h-char&gt;&lt;/span&gt;</span>cv2.createTrackbar(<span class="hljs-string">&#x27;H&#x27;</span>, <span class="hljs-string">&#x27;ChangeColor&#x27;</span>, <span class="hljs-number">140</span>, <span class="hljs-number">180</span>, <span class="hljs-keyword">lambda</span> x: <span class="hljs-literal">None</span>)<span class="hljs-comment"># cv2.createTrackbar(&#x27;S&#x27;, &#x27;ChangeColor&#x27;, 100, 255, lambda x: None)</span><span class="hljs-comment"># cv2.createTrackbar(&#x27;V&#x27;, &#x27;ChangeColor&#x27;, 100, 255, lambda x: None)</span><span class="hljs-comment"># 获取滑块的值</span>h = cv2.getTrackbarPos(<span class="hljs-string">&#x27;H&#x27;</span>, <span class="hljs-string">&#x27;ChangeColor&#x27;</span>)<span class="hljs-comment"># s = cv2.getTrackbarPos(&#x27;S&#x27;, &#x27;ChangeColor&#x27;)</span><span class="hljs-comment"># v = cv2.getTrackbarPos(&#x27;V&#x27;, &#x27;ChangeColor&#x27;)</span><span class="hljs-keyword">for</span> row <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(rows):    <span class="hljs-keyword">for</span> col <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(cols):        <span class="hljs-keyword">if</span> dilated[row, col] == <span class="hljs-number">255</span>:            img_hsv.itemset((row, col, <span class="hljs-number">0</span>), h)            <span class="hljs-comment"># img_hsv.itemset((row, col, 1), s)</span>            <span class="hljs-comment"># img_hsv.itemset((row, col, 2), v)</span>img = cv2.cvtColor(img_hsv, cv2.COLOR_HSV2BGR)cv2.imshow(<span class="hljs-string">&quot;ChangeColor&quot;</span>, img)</code></pre></div><h4 id="完整代码">完整代码</h4><p>可以直接在我的Github上Clone<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><a href="https://github.com/catchcodes/ChangeCloth">https://github.com/catchcodes/ChangeCloth</a></p><div class="highlight"><pre class="code"><code><span class="hljs-comment">#!/usr/bin/env python</span><span class="hljs-comment"># -*- coding:utf-8 -*-</span><span class="hljs-comment"># @FileName  :main.py</span><span class="hljs-comment"># @Time      :2023/4/7 19:46</span><span class="hljs-comment"># @Author    :Ouyang Bin</span><span class="hljs-keyword">import</span> cv2<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-comment"># 不存在的HSV值</span>HSV = [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">256</span>]newHSV = [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">256</span>]<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bgr2hsv</span>(<span class="hljs-params">b, g, r</span>):</span>    <span class="hljs-comment"># 限定像素BGR值的范围</span>    <span class="hljs-keyword">assert</span> (<span class="hljs-number">0</span> &lt;= r &lt; <span class="hljs-number">256</span> <span class="hljs-keyword">and</span> <span class="hljs-number">0</span> &lt;= g &lt; <span class="hljs-number">256</span> <span class="hljs-keyword">and</span> <span class="hljs-number">0</span> &lt;= b &lt; <span class="hljs-number">256</span>)    r, g, b = r / <span class="hljs-number">255.0</span>, g / <span class="hljs-number">255.0</span>, b / <span class="hljs-number">255.0</span>    Max = <span class="hljs-built_in">max</span>(r, g, b)    Min = <span class="hljs-built_in">min</span>(r, g, b)    D = Max - Min    HSV[<span class="hljs-number">2</span>] = <span class="hljs-built_in">int</span>(Max * <span class="hljs-number">255</span>)    HSV[<span class="hljs-number">1</span>] = <span class="hljs-built_in">int</span>(<span class="hljs-number">255</span> * (D / Max)) <span class="hljs-keyword">if</span> Max != <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span>    <span class="hljs-keyword">if</span> D == <span class="hljs-number">0</span>:        HSV[<span class="hljs-number">0</span>] = <span class="hljs-number">0</span>    <span class="hljs-keyword">elif</span> Max == r:        HSV[<span class="hljs-number">0</span>] = (<span class="hljs-number">60</span> * ((g - b) / D)) % <span class="hljs-number">360</span>    <span class="hljs-keyword">elif</span> Max == g:        HSV[<span class="hljs-number">0</span>] = (<span class="hljs-number">120</span> + <span class="hljs-number">60</span> * ((b - r) / D)) % <span class="hljs-number">360</span>    <span class="hljs-keyword">elif</span> Max == b:        HSV[<span class="hljs-number">0</span>] = (<span class="hljs-number">240</span> + <span class="hljs-number">60</span> * ((r - g) / D)) % <span class="hljs-number">360</span>    HSV[<span class="hljs-number">0</span>] = <span class="hljs-built_in">int</span>(HSV[<span class="hljs-number">0</span>] / <span class="hljs-number">2</span>)    <span class="hljs-keyword">return</span> HSV<span class="hljs-comment"># 回调函数</span><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_color_BGR</span>(<span class="hljs-params">envent, x, y, flags, param</span>):</span>    <span class="hljs-keyword">if</span> envent == cv2.EVENT_LBUTTONDOWN:        <span class="hljs-comment"># 注意&lt;span class=&quot;bd-box&quot;&gt;&lt;h-char class=&quot;bd bd-beg&quot;&gt;&lt;h-inner&gt;：&lt;/h-inner&gt;&lt;/h-char&gt;&lt;/span&gt;opencv的行列与坐标相反(row, col) -&gt; (y, x)</span>        BGR = img[y, x].tolist()        bgr2hsv(BGR[<span class="hljs-number">0</span>], BGR[<span class="hljs-number">1</span>], BGR[<span class="hljs-number">2</span>])img = cv2.imread(<span class="hljs-string">r&quot;C:\Users\19941\Pictures\cloth.jpg&quot;</span>)img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)rows, cols, _ = img_hsv.shapecv2.namedWindow(<span class="hljs-string">&quot;ChangeColor&quot;</span>)cv2.resizeWindow(<span class="hljs-string">&quot;ChangeColor&quot;</span>, rows, cols)cv2.setMouseCallback(<span class="hljs-string">&quot;ChangeColor&quot;</span>, get_color_BGR)cv2.createTrackbar(<span class="hljs-string">&#x27;H&#x27;</span>, <span class="hljs-string">&#x27;ChangeColor&#x27;</span>, <span class="hljs-number">140</span>, <span class="hljs-number">180</span>, <span class="hljs-keyword">lambda</span> x: <span class="hljs-literal">None</span>)<span class="hljs-comment"># cv2.createTrackbar(&#x27;S&#x27;, &#x27;ChangeColor&#x27;, 100, 255, lambda x: None)</span><span class="hljs-comment"># cv2.createTrackbar(&#x27;V&#x27;, &#x27;ChangeColor&#x27;, 100, 255, lambda x: None)</span>times = <span class="hljs-number">0</span><span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:    cv2.imshow(<span class="hljs-string">&#x27;ChangeColor&#x27;</span>, img)    <span class="hljs-keyword">if</span> newHSV != HSV:        <span class="hljs-keyword">if</span> times == <span class="hljs-number">0</span>:            <span class="hljs-comment"># HSV 的下界限</span>            lower = np.array([HSV[<span class="hljs-number">0</span>] - <span class="hljs-number">7</span>, <span class="hljs-number">50</span>, <span class="hljs-number">50</span>])            <span class="hljs-comment"># HSV 的上界限</span>            upper = np.array([HSV[<span class="hljs-number">0</span>] + <span class="hljs-number">7</span>, <span class="hljs-number">255</span>, <span class="hljs-number">255</span>])            mask = cv2.inRange(img_hsv, lower, upper)            <span class="hljs-comment"># 核</span>            kernel = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]], dtype=np.uint8)            <span class="hljs-comment"># 腐蚀图像&lt;span class=&quot;bd-box&quot;&gt;&lt;h-char class=&quot;bd bd-beg&quot;&gt;&lt;h-inner&gt;，&lt;/h-inner&gt;&lt;/h-char&gt;&lt;/span&gt;除去干扰点</span>            eroded = cv2.erode(mask, kernel, iterations=<span class="hljs-number">1</span>)            <span class="hljs-comment"># 膨胀图像</span>            dilated = cv2.dilate(eroded, kernel, iterations=<span class="hljs-number">1</span>)            times += <span class="hljs-number">1</span>        h = cv2.getTrackbarPos(<span class="hljs-string">&#x27;H&#x27;</span>, <span class="hljs-string">&#x27;ChangeColor&#x27;</span>)        <span class="hljs-comment"># s = cv2.getTrackbarPos(&#x27;S&#x27;, &#x27;ChangeColor&#x27;)</span>        <span class="hljs-comment"># v = cv2.getTrackbarPos(&#x27;V&#x27;, &#x27;ChangeColor&#x27;)</span>        <span class="hljs-keyword">for</span> row <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(rows):            <span class="hljs-keyword">for</span> col <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(cols):                <span class="hljs-keyword">if</span> dilated[row, col] == <span class="hljs-number">255</span>:                    img_hsv.itemset((row, col, <span class="hljs-number">0</span>), h)                    <span class="hljs-comment"># img_hsv.itemset((row, col, 1), s)</span>                    <span class="hljs-comment"># img_hsv.itemset((row, col, 2), v)</span>        img = cv2.cvtColor(img_hsv, cv2.COLOR_HSV2BGR)        cv2.imshow(<span class="hljs-string">&quot;ChangeColor&quot;</span>, img)    <span class="hljs-keyword">else</span>:        img = cv2.cvtColor(img_hsv, cv2.COLOR_HSV2BGR)        cv2.imshow(<span class="hljs-string">&quot;ChangeColor&quot;</span>, img)    <span class="hljs-keyword">if</span> cv2.waitKey(<span class="hljs-number">1</span>) &amp; <span class="hljs-number">0xff</span> == <span class="hljs-number">27</span>:        <span class="hljs-keyword">break</span>cv2.destroyAllWindows()</code></pre></div>]]></content>
    
    
    <summary type="html">&lt;img title=&quot;&quot; src=&quot;https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/31.jpg&quot; alt=&quot;&quot; data-align=&quot;center&quot; width=&quot;477&quot;&gt;</summary>
    
    
    
    
    <category term="OpenCV" scheme="https://catchcodes.github.io/tags/OpenCV/"/>
    
    <category term="数字图像处理" scheme="https://catchcodes.github.io/tags/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
    <category term="Python" scheme="https://catchcodes.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Markdown教程</title>
    <link href="https://catchcodes.github.io/Articles/3229213266.html"/>
    <id>https://catchcodes.github.io/Articles/3229213266.html</id>
    <published>2023-04-06T11:51:36.000Z</published>
    <updated>2023-04-07T10:30:16.477Z</updated>
    
    <content type="html"><![CDATA[<img title="" src="https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/7.jpg" alt="" data-align="center" width="555"><span id="more"></span><h2 id="前言">前言</h2><p><span class="bd-box"><h-char class="bd bd-end"><h-inner>“</h-inner></h-char></span> 工欲善其事<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>必先利其器<span class="bd-box"><h-char class="bd bd-beg"><h-inner>”</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><p>书写博客内容或项目的README文档是一个繁琐的过程<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>一个好的工具能让简化这个过程<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>而Markdown绝对是最好的工具之一<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我将用这一篇博客带你入门Markdown的语法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><h3 id="markdown简单介绍">Markdown简单介绍</h3><p>引自Wikipedia<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p><blockquote><p><strong>Markdown</strong>是一种轻量级标记语言<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>创始人为<a href="https://zh.wikipedia.org/wiki/%E7%B4%84%E7%BF%B0%C2%B7%E6%A0%BC%E9%AD%AF%E4%BC%AF" title="约翰·格鲁伯">约翰·格鲁伯</a><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>它允许人们使用易读易写的纯文本格式编写文档<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>然后转换成有效的<a href="https://zh.wikipedia.org/wiki/XHTML" title="XHTML">XHTML</a><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>或者<a href="https://zh.wikipedia.org/wiki/HTML" title="HTML">HTML</a><span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>文档<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这种语言吸收了很多在电子邮件中已有的纯文本标记的特性<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><p>由于Markdown的轻量化<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>易读易写特性<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>并且对于图片<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>图表<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>数学式都有支持<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>目前许多网站都广泛使用Markdown来撰写帮助文档或是用于论坛上发表消息<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>如<a href="https://zh.wikipedia.org/wiki/GitHub" title="GitHub">GitHub</a><span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span><a href="https://zh.wikipedia.org/wiki/Reddit" title="Reddit">Reddit</a><span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span><a href="https://zh.wikipedia.org/wiki/Discord" title="Discord">Discord</a><span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span><a href="https://zh.wikipedia.org/wiki/Diaspora_(%E7%A4%BE%E4%BA%A4%E7%BD%91%E7%BB%9C)" title="Diaspora (社交网络)">Diaspora</a><span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span><a href="https://zh.wikipedia.org/wiki/Stack_Exchange" title="Stack Exchange">Stack Exchange</a><span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span><a href="https://zh.wikipedia.org/wiki/OpenStreetMap" title="OpenStreetMap">OpenStreetMap</a> <span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span><a href="https://zh.wikipedia.org/wiki/SourceForge" title="SourceForge">SourceForge</a><span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span><a href="https://zh.wikipedia.org/wiki/%E7%AE%80%E4%B9%A6" title="简书">简书</a>等<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>甚至还能被用来撰写电子书<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p></blockquote><p>Markdown可用于在纯文本文档中添加格式化元素<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>让使用者更加专注于文字内容<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>能轻松在码字的同时做出美观大方的排版<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>不用受到格式上的桎梏<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><h3 id="markdown工作原理">Markdown工作原理</h3><p><strong>Markdown编辑器</strong><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>应用程序<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>通过<strong>Markdown处理器</strong>将markdown格式的文本转换为html格式<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>从而在Web上显示</p><img title="" src="https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/Markdown%E5%8E%9F%E7%90%86.png" alt="" data-align="center" width="283"><p>Markdown 格式的文件转换为 HTML 或可打印的文档的步骤如下<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p><blockquote><ol><li>使用文本编辑器或 Markdown 专用的应用程序创建 Markdown 文件<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>该文件应带有 <code>.md</code> 或 <code>.markdown</code> 扩展名<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></li><li>在 Markdown 应用程序中打开 Markdown 文件<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></li><li>使用 Markdown 应用程序将 Markdown 文件转换为 HTML 文档<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></li><li>在 web 浏览器中查看 HTML 文件<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>或使用 Markdown 应用程序将其转换为其他文件格式<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>例如 PDF<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></li></ol></blockquote><h3 id="markdown基础教程">Markdown基础教程</h3><p>个人比较推荐直接看Markdown官方教程<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>本文仅是Markdown一个粗略的教程<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><div align=center><a href="https://markdown.com.cn/"><img src="https://markdown.com.cn/hero.png" /></a></br>Markdown官方教程</div><table><thead><tr><th style="text-align:center">元素</th><th style="text-align:left">Markdown语法</th><th style="text-align:center">说明</th></tr></thead><tbody><tr><td style="text-align:center">标题</td><td style="text-align:left"><code># H1&lt;/br&gt;## H2&lt;/br&gt;### H3 </code></td><td style="text-align:center">要创建标题<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>请在单词或短语前面添加井号(<code>#</code>) <span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><code>#</code>的<strong>数量</strong>代表了标题的级别</td></tr><tr><td style="text-align:center">段落</td><td style="text-align:left">空白行隔开即可</td><td style="text-align:center">要创建段落<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>请使用<strong>空白行</strong>将一行或多行文本进行分隔<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>不要用空格<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>spaces<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>或制表符<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span> tabs<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>缩进段落</td></tr><tr><td style="text-align:center">换行</td><td style="text-align:left">空格+回车即可</td><td style="text-align:center">在一行的末尾添加<strong>两个或多个</strong>空格<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>然后按回车键,即可换行<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></td></tr><tr><td style="text-align:center"><strong>粗体</strong></td><td style="text-align:left">** 内容 **</td><td style="text-align:center">要加粗文本<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>请在单词或短语的前后各添加<strong>两个星号</strong><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span><code>**</code><span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span></td></tr><tr><td style="text-align:center"><em>斜体</em></td><td style="text-align:left">* 内容 *</td><td style="text-align:center">要用斜体显示文本<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>请在单词或短语前后添加<strong>一个星号</strong>(<code>*</code>)</td></tr><tr><td style="text-align:center"><s>删除线</s></td><td style="text-align:left">~~ 要删除内容 ~~</td><td style="text-align:center">在单词前后使用<strong>两个波浪号~~</strong></td></tr><tr><td style="text-align:center">块引用</td><td style="text-align:left">&gt; 引用内容</td><td style="text-align:center">要创建块引用<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>请在段落前添加<strong>一个 <code>&gt;</code></strong> 符号</td></tr><tr><td style="text-align:center">多段落引用</td><td style="text-align:left">&gt;段落1</br>&gt; </br>&gt;段落2</td><td style="text-align:center">段落之间的<strong>空白行</strong>添加一个<code>&gt;</code> 符号</td></tr><tr><td style="text-align:center">嵌套块引用</td><td style="text-align:left">&gt;内容</br>&gt;&gt;嵌套段落</td><td style="text-align:center">在要嵌套的段落前添加<strong>一个 <code>&gt;&gt;</code></strong> 符号</td></tr><tr><td style="text-align:center">带其他元素的块引用</td><td style="text-align:left">在引用块中正常加入即可</td><td style="text-align:center">块引用可以包含其他 Markdown 格式的元素<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>大部分都能<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></td></tr><tr><td style="text-align:center">有序列表</td><td style="text-align:left">1. 第一项</br>2. 第二项</td><td style="text-align:center">要创建有序列表<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>请在每个列表项前添加数字并紧跟一个英文句点<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>数字<strong>不必按数学顺序</strong>排列<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但是列表应当以数字 1 起始<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></td></tr><tr><td style="text-align:center">无序列表</td><td style="text-align:left">- 列表项</br>或</br>* 列表项</br>或</br>+ 列表项</td><td style="text-align:center">要创建无序列表<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>请在每个列表项前面添加<strong>破折号</strong> (-)<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span><strong>星号</strong> (*) 或<strong>加号</strong> (+) <span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>缩进一个或多个列表项可创建嵌套列表<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></td></tr><tr><td style="text-align:center">带其他元素的列表</td><td style="text-align:left">在列表中正常加入即可</td><td style="text-align:center">要在保留列表连续性的同时在列表中添加另一种元素<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>请将该元素缩进四个空格或<strong>一个制表符</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></td></tr><tr><td style="text-align:center">代码</td><td style="text-align:left"><code> `代码` </code></td><td style="text-align:center">要将单词或短语表示为代码<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>请将其包裹在<strong>一对反引号 ``</strong> 中</td></tr><tr><td style="text-align:center">代码块</td><td style="text-align:left"><code>代码块</code></td><td style="text-align:center">在代码块之前和之后的行上使用<strong>三个反引号</strong>(```)或<strong>三个波浪号</strong><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>~~~<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>在代码块之前的反引号旁边指定一种语言可以对应<strong>高亮</strong></td></tr><tr><td style="text-align:center">分割线</td><td style="text-align:left">***</br>或—</br>或___</td><td style="text-align:center">要创建分隔线<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>请在单独一行上使用<strong>三个或多个</strong>星号 (***)<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>破折号 (—) 或下划线 (___) <span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>并且不能包含其他内容<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>为了兼容性<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>请在分隔线的前后均添加空白行<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></td></tr><tr><td style="text-align:center">链接</td><td style="text-align:left"><code>[超链接显示名](超链接地址 &quot;超链接title&quot;)</code></td><td style="text-align:center">链接文本放在<strong>中括号</strong>[]内<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>链接地址放在后面的<strong>括号</strong>()中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>链接title(悬停时出现的文字)可选</td></tr><tr><td style="text-align:center">网址和Email地址</td><td style="text-align:left">&lt;网址&gt;</td><td style="text-align:center">使用<strong>尖括号</strong>可以很方便地把URL或者email地址变成可点击的链接</td></tr><tr><td style="text-align:center">图片</td><td style="text-align:left"><code>![图片alt](图片链接 &quot;图片title&quot;)</code></td><td style="text-align:center">要添加图像<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>请使用<strong>感叹号</strong> (!), 然后在<strong>方括号</strong>增加替代文本<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>图片链接放在<strong>圆括号</strong>里<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>括号里的链接后可以增加一个可选的图片标题文本</td></tr><tr><td style="text-align:center">表格</td><td style="text-align:left"><code>|表头1|表头2|   | ---| ---|     |表项1|表项2|</code></td><td style="text-align:center">要添加表<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>请使用三个或多个连字符<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span><strong>—</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>创建每列的标题<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>并使用竖线(|)分隔每列<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></br>您可以选择在表的任一端添加竖线<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>通过在标题行中的连字符的<strong>左侧<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>右侧或两侧添加冒号</strong><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>:<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>将列中的文本<strong>对齐</strong>到左侧<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>右侧或中心<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></td></tr><tr><td style="text-align:center">任务列表</td><td style="text-align:left">- [] 未完成任务</br>- [x] 已完成任务</td><td style="text-align:center">要创建任务列表<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>请在任务列表项之前添加<strong>破折号-和方括号[ ]</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>并在[ ]前面加上<strong>空格</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>要选择一个复选框<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>请在方括号[x]之间添加 x</td></tr><tr><td style="text-align:center">引用类型链接</td><td style="text-align:left">第一部分<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>[文本] [数字或字母或标点符号]</br>第二部分<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>[第一部分第二个括号的内容]: <code>https://网址</code></td><td style="text-align:center">第一部分中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>第一组<strong>方括号</strong>包围应显示为链接的文本<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>第一组和第二组<strong>方括号</strong>之间包含一个空格<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>第二组括号显示了一个标签<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>该标签用于指向您存储在文档其他位置的链接<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></br>可以将链接的第二部分放在Markdown文档中的任何位置<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></br>第二部分中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>1.放在<strong>方括号</strong>中的标签<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>其后紧跟一个<strong>冒号</strong>和至少一个<strong>空格</strong><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>例如[label]: <span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>2.链接的URL<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>可以选择将其括在尖括号中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>3.链接的可选标题<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>可以将其括在双引号<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>单引号或括号中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></td></tr><tr><td style="text-align:center">脚注</td><td style="text-align:left">正文[^1]正文</br>[^1]: 脚注内容</td><td style="text-align:center">要创建脚注参考<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>请在方括号<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>[^1]<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>内添加插入符号和标识符<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>标识符可以是数字或单词<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但不能包含空格或制表符<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>在括号内使用另一个插入符号和数字添加脚注<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>并用冒号和文本<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>[^1]: 脚注内容.<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>您不必在文档末尾添加脚注<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>您可以将它们放在除列表<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>块引号和表之类的其他元素之外的任何位置<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></td></tr><tr><td style="text-align:center">emoji</td><td style="text-align:left">复制和粘贴表情符号😀</br>或</br>使用表情符号简码<code>:符号简码:</code></td><td style="text-align:center">以冒号开头和结尾<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>并包含表情符号的名称<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span><a href="https://gist.github.com/rxaviers/7360908">这是简码列表</a></td></tr></tbody></table><p>此外<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>Markdown还支持html语言<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>需要编辑器支持<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>例如<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p><div class="highlight"><pre class="code"><code><span class="hljs-comment">&lt;!--居中显示带链接的图片--&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">align</span>=<span class="hljs-string">center</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">a</span> <span class="hljs-attr">href</span>=<span class="hljs-string">&quot;链接&quot;</span>&gt;</span>    <span class="hljs-tag">&lt;<span class="hljs-name">img</span> <span class="hljs-attr">src</span>=<span class="hljs-string">&quot;图片链接&quot;</span>&gt;</span>  <span class="hljs-tag">&lt;/<span class="hljs-name">a</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span> </code></pre></div><h3 id="markdown公式">Markdown公式</h3><p>markdown的数学公式分为<strong>行内公式</strong>和<strong>行间公式</strong>(有的编辑器不支持)<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>使用行内公式会让你的公式和之前的文字处于同一行<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> 使用行间公式会对之前的文字换行<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>然后公式单独成行并居中显示<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><p>行内公式<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><code>$ Latex行内公式$</code></p><p>行间公式<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><code>$$ Latex行间公式$$</code></p><h5 id="常用公式">常用公式</h5><p><strong>上角标</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><code> $ x^n$</code><span class="markdown-them-math-inline">$\rightarrow x^2$</span></p><p><strong>下角标</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><code> $ x_n$</code><span class="markdown-them-math-inline">$\rightarrow x_n$</span></p><p><strong>上划线<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></strong><code>$ \overline&#123;abc&#125;$</code><span class="markdown-them-math-inline">$\rightarrow \overline{abc}$</span></p><p>下划线<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><code>$ \underline&#123;abc&#125;$</code><span class="markdown-them-math-inline">$\rightarrow \underline{abc}$</span></p><p><strong>竖线<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></strong><code>$ \vert$</code><span class="markdown-them-math-inline">$\rightarrow \vert$</span></p><p><strong>模值</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><code>$ \| i \| $</code><span class="markdown-them-math-inline">$\displaystyle \rightarrow \|i\|$</span></p><p>乘号<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><code>$ \times$</code><span class="markdown-them-math-inline">$\rightarrow \times$</span></p><p>除号<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><code>$ \div$</code><span class="markdown-them-math-inline">$\rightarrow \div$</span></p><p>点乘<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>内积<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><code>$ \odot 或 \cdot$</code><span class="markdown-them-math-inline">$\rightarrow \odot \ \cdot $</span></p><p>叉乘<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>交叉积<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><code>$ \otime 或 \times $</code><span class="markdown-them-math-inline">$\rightarrow \otimes\ \times$</span></p><p><strong>平方根</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><code>$ \sqrt&#123;x&#125;$</code><span class="markdown-them-math-inline">$\displaystyle \rightarrow \sqrt{x}$</span></p><p>注<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>{ }中的内容视为总体</p><p><strong>n次根</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><code>$ \sqrt[n]&#123;x&#125;$</code><span class="markdown-them-math-inline">$\displaystyle \rightarrow \sqrt[n]{x}$</span></p><p><strong>分式</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><code>$ \frac&#123;x&#125;&#123;y&#125;$</code><span class="markdown-them-math-inline">$\rightarrow \frac{x}{y}$</span></p><p><strong>求和</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><code>$ \sum_&#123;k=1&#125;^n$</code><span class="markdown-them-math-inline">$\rightarrow \sum_{k=1}^n$</span></p><p><strong>求积</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><code>$ \prod_&#123;k=1&#125;^n$</code><span class="markdown-them-math-inline">$\rightarrow \prod_{k=1}^n$</span></p><p><strong>积分</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><code>$ \int_a^b$</code><span class="markdown-them-math-inline">$\rightarrow \int_a^b$</span></p><p><strong>围线积分</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><code>$ \oint$</code><span class="markdown-them-math-inline">$\rightarrow \oint$</span></p><p><strong>二重积分</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><code>$ \iint$</code><span class="markdown-them-math-inline">$\rightarrow \iint$</span></p><p><strong>三重积分</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><code>$ \iiint$</code><span class="markdown-them-math-inline">$\rightarrow \iiint$</span></p><p><strong>箭头</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p><ul><li><p><code>左箭头$\leftarrow \Leftarrow$</code>   <span class="markdown-them-math-inline">$\leftarrow\ \ \ \Leftarrow$</span></p></li><li><p><code>右箭头$ \rightarrow \Rightarrow$</code>     <span class="markdown-them-math-inline">$\rightarrow\ \ \  \Rightarrow$</span></p></li><li><p><code>双向箭头$ \leftrightarrow \Leftrightarrow$</code>      <span class="markdown-them-math-inline">$\leftrightarrow\ \ \  \Leftrightarrow$</span></p></li></ul><p><strong>无穷</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><code>$ \infty$</code><span class="markdown-them-math-inline">$\rightarrow \infty$</span></p><p><strong>极限</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><code>$ \lim_&#123;0\rightarrow \infty&#125;</code> <span class="markdown-them-math-inline">$\rightarrow \lim_{0\rightarrow\infty}$</span></p><p><strong>偏导</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><code>$ \partial$</code> <span class="markdown-them-math-inline">$\rightarrow \partial$</span></p><p>哈密顿算子<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><code>$ \nabla$</code><span class="markdown-them-math-inline">$\rightarrow \nabla$</span></p><p>任意<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><code>$ \forall$</code><span class="markdown-them-math-inline">$\rightarrow \forall$</span></p><p>存在<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><code>$ \exists$</code><span class="markdown-them-math-inline">$\rightarrow \exists$</span></p><p><strong>正负号</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><code>$ \pm 或 \mp$</code><span class="markdown-them-math-inline">$\rightarrow \pm \ \mp$</span></p><p><strong>大于等于<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></strong><code>$ \geq$</code><span class="markdown-them-math-inline">$\rightarrow \geq$</span></p><p><strong>小于等于<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></strong><code>$ \leq$</code><span class="markdown-them-math-inline">$\rightarrow \leq$</span></p><p><strong>约等于</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><code>$ \approx$</code><span class="markdown-them-math-inline">$\rightarrow \approx$</span></p><p><strong>不等于</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><code>$ \neq$</code><span class="markdown-them-math-inline">$\rightarrow \neq$</span></p><p>属于<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><code>$ \in$</code><span class="markdown-them-math-inline">$\rightarrow \in$</span></p><p>不属于<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><code>$ \notin$</code><span class="markdown-them-math-inline">$\rightarrow \notin$</span></p><p>子集<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><code>$ \subseteq$</code><span class="markdown-them-math-inline">$\rightarrow \subseteq$</span></p><p>真子集<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><code>$ \subset$</code><span class="markdown-them-math-inline">$\rightarrow \subset$</span></p><p>角度度数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><code>$ \degree$</code><span class="markdown-them-math-inline">$\rightarrow 360°$</span></p><p>角度<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><code>$ \angle$</code><span class="markdown-them-math-inline">$\rightarrow \angle 90°$</span></p></br><p>注<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>可以通过展示格式控制Markdown公式的大小<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>如<code>\displaystyle</code>显示格式<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>公式较大</p><p><code>$ \displaystyle \sum_&#123;k=1&#125;^n \int_0^1$</code>展现为<span class="markdown-them-math-inline">$\displaystyle \sum_{k=1}^n \int_0^1$</span></p><p>而<code>$ \sum_&#123;k=1&#125;^n$</code>展现为 <span class="markdown-them-math-inline">$\sum_{k=1}^n \int_0^1$</span></p></br><p><strong>分段函数</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p><div class="highlight"><pre class="code"><code>$$ 函数名=\begin&#123;cases&#125;公式1 &amp; 条件1 \\公式2 &amp; 条件2 \\公式3 &amp; 条件3 \end&#123;cases&#125;$$</code></pre></div><p><code>&amp;</code>符号用于对齐<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>例如<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p><div class="highlight"><pre class="code"><code>$ W(x) = \begin&#123;cases&#125; (a+2)|x|^3-(a+3)|x|^2+1 &amp; |x| \leq 1 \\ a|x|^3-5a|x|^2+8a|x|-4a &amp; 1&lt;|x|&lt;2 \\ 0 &amp; otherwise \end&#123;cases&#125;$</code></pre></div><p><span class="markdown-them-math-inline">$ W(x) = \begin{cases} (a+2)|x|^3-(a+3)|x|^2+1 &amp; |x| \leq 1 \\ a|x|^3-5a|x|^2+8a|x|-4a &amp; 1&lt;|x|&lt;2 \\ 0 &amp; otherwise \end{cases}$</span></p><p><strong>矩阵</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p><div class="highlight"><pre class="code"><code>$\begin&#123;pmatrix&#125;0&amp;1&amp;0\\1&amp;1&amp;0\\0&amp;0&amp;1\\\end&#123;pmatrix&#125;$</code></pre></div><p>注1<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>pmatrix为小括号()包围<span class="bd-box"><h-char class="bd bd-beg"><h-inner>；</h-inner></h-char></span>bmatrix为中括号[]包围<span class="bd-box"><h-char class="bd bd-beg"><h-inner>；</h-inner></h-char></span>vmatrix为单竖线||包围<br>注2<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>\cdots为横省略元素<span class="bd-box"><h-char class="bd bd-beg"><h-inner>；</h-inner></h-char></span>\vdots为竖省略元素<span class="bd-box"><h-char class="bd bd-beg"><h-inner>；</h-inner></h-char></span>\ddots为斜省略元素</p><p>例如<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p><div class="highlight"><pre class="code"><code>$ D = \begin&#123;bmatrix&#125; 2 \\ &amp;3\\ &amp;&amp;2 \\ &amp;&amp;&amp;3 \\ &amp;&amp;&amp;&amp;2 \end&#123;bmatrix&#125;$$ \begin&#123;bmatrix&#125;&#123;a<span class="hljs-emphasis">_&#123;11&#125;&#125;&amp;&#123;a_</span>&#123;12&#125;&#125;&amp;&#123;\cdots&#125;&amp;&#123;a<span class="hljs-emphasis">_&#123;1n&#125;&#125;\\&#123;a_</span>&#123;21&#125;&#125;&amp;&#123;a<span class="hljs-emphasis">_&#123;22&#125;&#125;&amp;&#123;\cdots&#125;&amp;&#123;a_</span>&#123;2n&#125;&#125;\\&#123;\vdots&#125;&amp;&#123;\vdots&#125;&amp;&#123;\ddots&#125;&amp;&#123;\vdots&#125;\\&#123;a<span class="hljs-emphasis">_&#123;m1&#125;&#125;&amp;&#123;a_</span>&#123;m2&#125;&#125;&amp;&#123;\cdots&#125;&amp;&#123;a<span class="hljs-emphasis">_&#123;mn&#125;&#125;\\\end&#123;bmatrix&#125;$</span></code></pre></div><p><span class="markdown-them-math-inline">$ D = \begin{bmatrix} 2 \\ &amp;3\\ &amp;&amp;2 \\ &amp;&amp;&amp;3 \\ &amp;&amp;&amp;&amp;2 \end{bmatrix}$</span></p></br><p><span class="markdown-them-math-inline">$\begin{bmatrix}{a_{11}}&amp;{a_{12}}&amp;{\cdots}&amp;{a_{1n}}\\{a_{21}}&amp;{a_{22}}&amp;{\cdots}&amp;{a_{2n}}\\{\vdots}&amp;{\vdots}&amp;{\ddots}&amp;{\vdots}\\{a_{m1}}&amp;{a_{m2}}&amp;{\cdots}&amp;{a_{mn}}\\\end{bmatrix}$</span></p><h5 id="常用希腊字母">常用希腊字母</h5><table><thead><tr><th style="text-align:center">希腊字母</th><th style="text-align:left">小写</th><th style="text-align:left">大写</th><th>英语</th><th style="text-align:center">发音</th></tr></thead><tbody><tr><td style="text-align:center"><span class="markdown-them-math-inline">$\alpha\ A$</span></td><td style="text-align:left">\alpha</td><td style="text-align:left">A</td><td>alpha</td><td style="text-align:center"><strong>/'ælfə/</strong></td></tr><tr><td style="text-align:center"><span class="markdown-them-math-inline">$\beta \ B$</span></td><td style="text-align:left">\beta</td><td style="text-align:left">B</td><td>beta</td><td style="text-align:center"><strong>/'bi:tə/ 或 /'beɪtə/</strong></td></tr><tr><td style="text-align:center"><span class="markdown-them-math-inline">$\gamma \ \Gamma$</span></td><td style="text-align:left">\gamma</td><td style="text-align:left">\Gamma</td><td>gamma</td><td style="text-align:center"><strong>/'gæmə/</strong></td></tr><tr><td style="text-align:center"><span class="markdown-them-math-inline">$\pi \ \Pi$</span></td><td style="text-align:left">\pi</td><td style="text-align:left">\Pi</td><td>pi</td><td style="text-align:center"><strong>/paɪ/</strong></td></tr><tr><td style="text-align:center"><span class="markdown-them-math-inline">$\lambda \ \Lambda$</span></td><td style="text-align:left">\lambda</td><td style="text-align:left">\Lambda</td><td>lambda</td><td style="text-align:center"><strong>/'læmdə/</strong></td></tr><tr><td style="text-align:center"><span class="markdown-them-math-inline">$\delta \ \Delta$</span></td><td style="text-align:left">\delta</td><td style="text-align:left">\Delta</td><td>delta</td><td style="text-align:center"><strong>/'deltə/</strong></td></tr><tr><td style="text-align:center"><span class="markdown-them-math-inline">$\theta\ \Theta $</span></td><td style="text-align:left">\theta</td><td style="text-align:left">\Theta</td><td>theta</td><td style="text-align:center"><strong>/'θi:tə/</strong></td></tr><tr><td style="text-align:center"><span class="markdown-them-math-inline">$\rho \ P$</span></td><td style="text-align:left">\rho</td><td style="text-align:left">P</td><td>rho</td><td style="text-align:center"><strong>/rəʊ/</strong></td></tr><tr><td style="text-align:center"><span class="markdown-them-math-inline">$\sigma \ \Sigma$</span></td><td style="text-align:left">\sigma</td><td style="text-align:left">\Sigma</td><td>sigma</td><td style="text-align:center"><strong>/'sɪɡmə/</strong></td></tr><tr><td style="text-align:center"><span class="markdown-them-math-inline">$\epsilon \ E$</span></td><td style="text-align:left">\epsilon</td><td style="text-align:left">E</td><td>epsilon</td><td style="text-align:center"><strong>/'epsɪlɒn/</strong></td></tr><tr><td style="text-align:center"><span class="markdown-them-math-inline">$\zeta \ Z$</span></td><td style="text-align:left">\zeta</td><td style="text-align:left">Z</td><td>zeta</td><td style="text-align:center"><strong>/'zi:tə/</strong></td></tr><tr><td style="text-align:center"><span class="markdown-them-math-inline">$\eta\ H$</span></td><td style="text-align:left">\eta</td><td style="text-align:left">H</td><td>eta</td><td style="text-align:center"><strong>/'i:tə/</strong></td></tr><tr><td style="text-align:center"><span class="markdown-them-math-inline">$\phi \ \Phi$</span></td><td style="text-align:left">\phi</td><td style="text-align:left">\Phi</td><td>phi</td><td style="text-align:center"><strong>/faɪ/</strong></td></tr><tr><td style="text-align:center"><span class="markdown-them-math-inline">$\psi \Psi$</span></td><td style="text-align:left">\psi</td><td style="text-align:left">\Psi</td><td>psi</td><td style="text-align:center"><strong>/psaɪ/</strong></td></tr><tr><td style="text-align:center"><span class="markdown-them-math-inline">$\omega\ \Omega$</span></td><td style="text-align:left">\omega</td><td style="text-align:left">\Omega</td><td>omega</td><td style="text-align:center"><strong>/'əʊmɪɡə/ 或 /oʊ’meɡə/</strong></td></tr><tr><td style="text-align:center"><span class="markdown-them-math-inline">$\tau \ T$</span></td><td style="text-align:left">\tau</td><td style="text-align:left">T</td><td>tau</td><td style="text-align:center"><strong>/tɔ:/or/taʊ/</strong></td></tr><tr><td style="text-align:center"><span class="markdown-them-math-inline">$\kappa\ K$</span></td><td style="text-align:left">\kappa</td><td style="text-align:left">K</td><td>kappa</td><td style="text-align:center"><strong>/'kæpə/</strong></td></tr><tr><td style="text-align:center"><span class="markdown-them-math-inline">$\mu\ M$</span></td><td style="text-align:left">\mu</td><td style="text-align:left">M</td><td>mu</td><td style="text-align:center"><strong>/mju:/</strong></td></tr><tr><td style="text-align:center"><span class="markdown-them-math-inline">$\nu \ N$</span></td><td style="text-align:left">\nu</td><td style="text-align:left">N</td><td>nu</td><td style="text-align:center"><strong>/nju:/</strong></td></tr><tr><td style="text-align:center"><span class="markdown-them-math-inline">$\xi\ \Xi$</span></td><td style="text-align:left">\xi</td><td style="text-align:left">\Xi</td><td>xi</td><td style="text-align:center"><strong>/ksi/ 或 /'zaɪ/ 或 /'ksaɪ/</strong></td></tr><tr><td style="text-align:center"><span class="markdown-them-math-inline">$\omicron\ O$</span></td><td style="text-align:left">\omicron</td><td style="text-align:left">O</td><td>omicron</td><td style="text-align:center"><strong>/əu’maikrən/ 或 /'ɑmɪ,krɑn/</strong></td></tr><tr><td style="text-align:center"><span class="markdown-them-math-inline">$\upsilon \ \Upsilon$</span></td><td style="text-align:left">\upsilon</td><td style="text-align:left">\Upsilon</td><td>upsilon</td><td style="text-align:center"><strong>/'ipsilon/ 或 /'ʌpsilɒn/</strong></td></tr><tr><td style="text-align:center"><span class="markdown-them-math-inline">$\chi \ X $</span></td><td style="text-align:left">\chi</td><td style="text-align:left">X</td><td>chi</td><td style="text-align:center"><strong>/kaɪ/</strong></td></tr></tbody></table><h5 id="常用函数名">常用函数名</h5><p><img src="https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/%E5%87%BD%E6%95%B0.png" alt=""></p>]]></content>
    
    
    <summary type="html">&lt;img title=&quot;&quot; src=&quot;https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/7.jpg&quot; alt=&quot;&quot; data-align=&quot;center&quot; width=&quot;555&quot;&gt;</summary>
    
    
    
    
    <category term="工具" scheme="https://catchcodes.github.io/tags/%E5%B7%A5%E5%85%B7/"/>
    
    <category term="markdown" scheme="https://catchcodes.github.io/tags/markdown/"/>
    
    <category term="教程" scheme="https://catchcodes.github.io/tags/%E6%95%99%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>Markdown编辑器——marktext下载与汉化</title>
    <link href="https://catchcodes.github.io/Articles/1377691752.html"/>
    <id>https://catchcodes.github.io/Articles/1377691752.html</id>
    <published>2023-04-05T05:06:26.000Z</published>
    <updated>2023-04-12T11:18:18.891Z</updated>
    
    <content type="html"><![CDATA[<img title="" src="https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/13.jpg" alt="" data-align="center" width="576"><span id="more"></span><h2 id="前言">前言</h2><p>在2021年末<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>Typora正式发布稳定版<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>开启收费模式<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>89￥买断三台设备<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>奈何本人是白嫖怪<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>没有知识服务付费的习惯<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>于是果断弃坑Typora<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我陆续用过VScode和uTools的Markdown插件去写MD<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但是这种插件功能也不是很强大<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>直到使用了MarkText<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>一款功能强大的开源Markdown文本编辑器<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><h2 id="下载marktext">下载MarkText</h2><div align=center><a href="https://github.com/marktext/marktext/releases"><img src="https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/marktext.png" /></a></br>MarkText官方下载地址</div><p>如果Github被墙了<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>可以试试这个链接 <a href="https://cowtransfer.com/s/14e71760dc564b">https://cowtransfer.com/s/14e71760dc564b</a></p><h2 id="汉化">汉化</h2><p>1<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>下载汉化后的app.asar <span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span> <a href="https://catchcodes.github.io/app.asar">https://catchcodes.github.io/app.asar</a></p><p>2<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>用汉化后的app.asar文件替换MarkText/resources中的app.asar</p><p>3<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>重启MarkText即可</p><img title="" src="https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/20230405135304.png" alt="" data-align="center" width="484">]]></content>
    
    
    <summary type="html">&lt;img title=&quot;&quot; src=&quot;https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/13.jpg&quot; alt=&quot;&quot; data-align=&quot;center&quot; width=&quot;576&quot;&gt;</summary>
    
    
    
    
    <category term="随笔" scheme="https://catchcodes.github.io/tags/%E9%9A%8F%E7%AC%94/"/>
    
    <category term="工具" scheme="https://catchcodes.github.io/tags/%E5%B7%A5%E5%85%B7/"/>
    
    <category term="markdown" scheme="https://catchcodes.github.io/tags/markdown/"/>
    
    <category term="marktext" scheme="https://catchcodes.github.io/tags/marktext/"/>
    
  </entry>
  
  <entry>
    <title>深度学习基础-数据处理</title>
    <link href="https://catchcodes.github.io/Articles/3233011741.html"/>
    <id>https://catchcodes.github.io/Articles/3233011741.html</id>
    <published>2023-04-03T10:23:54.000Z</published>
    <updated>2023-04-07T10:22:27.607Z</updated>
    
    <content type="html"><![CDATA[<img title="" src="https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/37.jpg" alt="" data-align="center" width="537"><span id="more"></span><h1 id="数据部分">数据部分</h1><p>所谓AI三要素<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>算力<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>算法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>数据<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><img src="https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/AI%E4%B8%89%E8%A6%81%E7%B4%A0.png" title="" alt="" data-align="center"><p>对于深度学习来说<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>模型需要大量不同种的数据(暂且先不谈小样本学习Few-shot Learing)<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>才能训练出正确率高<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>泛化性好的模型<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>而数据的质量很大程度上决定了模型的性能与鲁棒性<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><blockquote><p>Huber从稳健统计的角度系统地给出了鲁棒性3个层面的概念<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p><ol><li><p>模型具有较高的精度或有效性<span class="bd-box"><h-char class="bd bd-beg"><h-inner>；</h-inner></h-char></span></p></li><li><p>对于模型假设出现的较小偏差<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>只能对算法性能产生较小的影响<span class="bd-box"><h-char class="bd bd-beg"><h-inner>；</h-inner></h-char></span>噪声点</p></li><li><p>对于模型假设出现的较大偏差<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>不可对算法性能产生<span class="bd-box"><h-char class="bd bd-end"><h-inner>“</h-inner></h-char></span>灾难性<span class="bd-box"><h-char class="bd bd-beg"><h-inner>”</h-inner></h-char></span>的影响<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>离群点</p></li></ol></blockquote><p><em>题外话</em><span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span> ChatGPT的训练集60%是互联网爬取<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>也没管网站的Robots协议<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>20%是OpenAI的WebText2数据集<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>剩下的来自书或Wiki<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>一共4100亿的tokens<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>词元<span class="bd-box"><h-char class="bd bd-beg"><h-inner>？</h-inner></h-char><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>国内的大模型(Foundation Model)的训练集数据主要就是爬取网页和搜索引擎<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>数据质量良莠不齐<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>训练出的模型也只在中文处理能力上跟ChatGPT差不多<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>国内大模型下一步的重点应该放在数据上<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>而不是去扩大参数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><img src="https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/20230403203701.png" title="" alt="" data-align="center"><p>数据处理的过程主要分为<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p><p><strong>数据采集</strong>–&gt;<strong>数据标注</strong>–&gt;<strong>数据增强</strong>–&gt;<strong>数据清洗</strong></p><p>本文将主要介绍数据增强与数据清洗</p><h2 id="数据采集(data-acquisition)">数据采集(Data Acquisition)</h2><p>在深度学习中所使用的数据<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>其本身是存在于现实世界的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>是杂乱的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>随机的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>数量是无法确定的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>因为现实世界中无时无刻都在产生各种不同的新数据<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>而我们要使用的数据一般是结构化的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>有规律的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>确定数量的数据<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>所以一般数据采集就是从现实世界的数据中进行采样<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这一过程就是完成从现实世界把数据转移到我们当前任务环境中的过程<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>值得注意的是<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>在采样过程中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>要保证采样的数据分布规律和现实世界中的数据分布规律一样<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>因为只有这样<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>采样得到的数据在一定程度上才能能够代替现实世界中的数据<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><p>数据采集的方法主要有<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p><ul><li><p><strong>网络数据采集</strong><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>网络爬虫或公开数据<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span></p></li><li><p><strong>感知设备采集</strong><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>摄像头<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>传感器或其他采集设备<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span></p></li><li><p><strong>生成数据</strong><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>GAN<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>VAE或扩散模型<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span></p></li></ul><h2 id="数据标注(data-annotation)">数据标注(Data Annotation)</h2><p>数据标注是对未经处理过的语音<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>图片<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>文本<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>视频等数据进行加工处理<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>从而转变成机器可识别信息的过程<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>目前主流的模型都是监督学习<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>数据与标签一一映射才能参与训练<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><p>数据标准一般是借助标注工具来完成<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>比如<a href="https://github.com/wkentaro/labelme">labelme</a><span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span><a href="https://labelbox.com/">Labelbox</a><span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span><a href="https://github.com/opencv/cvat">CVAT</a><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>或者是通过数据标注众包<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>百度众包之类的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>标注</p><h2 id="数据增强(data-augmentation)">数据增强(Data Augmentation)</h2><p>数据增强指通过对已有数据添加微小改动或从已有数据新创建合成数据<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>以<strong>增加数据量</strong>的方法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><p>数据增强是对数据采集的补充<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>如果数据在采样过程中就已经包括了各种复杂环境下的数据<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>那么数据增强不是必要的<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>但是这可以增加你数据集中相关数据的数据量<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>然而采集到的数据一般都是不足以表达实际数据分布情况的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>数据增强的本质原因是由于原始数据无论是从数量还是质量上来说都不够丰富<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><p><strong>CV领域</strong>的数据增强主要有<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p><ul><li><p>空间几何变换<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>翻转<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>水平和垂直<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>随机裁剪<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>旋转<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>仿射变换<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>视觉变换<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>四点透视变换<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span></p></li><li><p>光照变换<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>随机亮度变换<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>对比度变换<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>色彩度变换<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>饱和度变换<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>噪声变换</p></li><li><p>遮挡变换<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><strong>Random erase</strong><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>随机删除<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>在图上随机遮挡某一部分的像素<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span><strong>Cutout</strong><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>裁剪<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>按照一定的间隔遮挡NxN像素大小的小格子<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>是具有规律的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>一般是等距间隔的小格子<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>N一般取2或4很小的值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>类似于给图片加噪声<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span><strong>hide and seek</strong><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>裁剪<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>按照更大的间隔遮挡NxN像素大小的小格子<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>是具有规律的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>一般是等距间隔的小格子<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>N一般取值更大一些<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span><strong>Grid Mask</strong><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>网格掩码<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>先把图像进行分成不同的格子<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>然后按照一定的方法去挑选遮挡某些格子<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>遮挡的效果取决于格子的大小和被遮挡的格子数量<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>类似于增加正样本的权重<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span><strong>Dropblock</strong><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>对图像数据使用dropout<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>然后再将多个dropout之间连成块<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>以此达到遮挡的目的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p></li><li><p>模糊类<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>高斯模糊<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>ElasticTransformation</p></li><li><p>HSV对比度变换</p></li><li><p>图像融合操作</p></li></ul><img title="" src="https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA.jpg" alt="" data-align="center" width="451"><p><strong>NLP领域</strong>的数据增强主要有<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p><ul><li><p>同义词替换</p></li><li><p>随机drop<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>对于标题和描述中的字或词,随机的进行删除,用空格代替</p></li><li><p>随机 shuffle, 即打乱词序</p></li><li><p>回译<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>快速产生一些不那么准确的翻译结果达到增加数据的目的</p><img title="" src="https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/%E5%9B%9E%E8%AF%91.png" alt="" width="380" data-align="center"></li><li><p>添加一些随机单词的拼写错误</p></li></ul><p>实际上<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>只有当我们增强后的样本与测试样本<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>应用场景<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>分布一致时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>才会达到性能最好<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>大部分模型在做了数据增强后<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>在最终的效果上还是有很大的提升的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这是因为数据增强的方法中经常会加入一些极端情况下的数据<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>而这种极端情况下的数据在现实环境中一般很难采集得到<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>但是也有一些小模型在做了扰动过大的数据增强后<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>在最终识别效果反而会下降</p><h2 id="数据清洗(cleansing)">数据清洗(Cleansing)</h2><p>数据清洗常用的过程有<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>缺失值处理<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>删除重复值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>异常值处理<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>归一化<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>标准化<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>白化处理<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><p>笔者主要介绍归一化<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>标准化与白化<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p><h4 id="归一化">归一化</h4><p>把数据变成(0,1)或者(-1,1<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>之间的小数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>把有量纲表达式变成<strong>无量纲表达式</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>便于不同单位或量级的指标能够进行比较和加权<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span><br>如<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p><ul><li>Min-Max归一化<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><span class="markdown-them-math-inline">$ \displaystyle x' = \frac{x - X_{min}}{X_{max} - X_{min}}$</span></li><li>平均归一化<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><span class="markdown-them-math-inline">$ \displaystyle x' = \frac{x - \mu}{X_{max} - X_{min}}$</span></li><li>对数函数转换<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><span class="markdown-them-math-inline">$ \displaystyle x' = log_n(x)$</span></li><li>反余切函数转换<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><span class="markdown-them-math-inline">$ \displaystyle x' = arctan(x) *\frac{2}{\pi}$</span></li></ul><h4 id="标准化">标准化</h4><p>使每个特征中的数值<em>平均值变为0</em>(将每个特征的值都减掉原始数据中该特征的平均)<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>标准差变为1<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>数据存在<strong>异常值</strong>和噪声时常用<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span><br>如<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p><ul><li>Z-score标准化<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span><span class="markdown-them-math-inline">$ \displaystyle x' = \frac{x-\mu}{\sigma}$</span></li></ul><p>归一化/标准化后可以加快梯度下降的求解速度<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>即提升模型的收敛速度<span class="bd-box"><h-char class="bd bd-beg"><h-inner>；</h-inner></h-char></span><br>一些分类器需要计算样本之间的距离(如欧氏距离)<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>例如<a href="https://zh.wikipedia.org/zh-cn/K-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95">KNN</a><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>如果一个特征值域范围非常大<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>那么距离计算就主要取决于这个特征<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>模型的泛化性降低</p><h4 id="白化处理">白化处理</h4><p>白化的目的是去除输入数据的冗余信息<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>输入数据集X<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>经过白化处理后<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>新的数据X’满足两个性质<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p><ul><li>特征之间相关性较低<span class="bd-box"><h-char class="bd bd-beg"><h-inner>；</h-inner></h-char></span></li><li>所有特征具有相同的方差<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></li></ul>]]></content>
    
    
    <summary type="html">&lt;img title=&quot;&quot; src=&quot;https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/37.jpg&quot; alt=&quot;&quot; data-align=&quot;center&quot; width=&quot;537&quot;&gt;</summary>
    
    
    
    
    <category term="Deep-Learing" scheme="https://catchcodes.github.io/tags/Deep-Learing/"/>
    
    <category term="深度学习" scheme="https://catchcodes.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="数据处理" scheme="https://catchcodes.github.io/tags/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>First Article</title>
    <link href="https://catchcodes.github.io/Articles/403018703.html"/>
    <id>https://catchcodes.github.io/Articles/403018703.html</id>
    <published>2023-04-02T11:34:48.000Z</published>
    <updated>2023-04-06T11:38:14.649Z</updated>
    
    <content type="html"><![CDATA[<h2 id="本人的第一篇文章">本人的第一篇文章</h2><img title="" src="https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/5.jpg" alt="" data-align="center" width="508"><span id="more"></span><p>本博客将记录笔者的学习总结<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>包括且不限于#深度学习 #高等数学 #C++ #Python #Julia</p></br><p>敬请期待<span class="bd-box"><h-char class="bd bd-beg"><h-inner>！</h-inner></h-char></span></p></br><p>——2023年4月2日</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;本人的第一篇文章&quot;&gt;本人的第一篇文章&lt;/h2&gt;
&lt;img title=&quot;&quot; src=&quot;https://cdn.jsdelivr.net/gh/catchcodes/MyImg/markdown_img/5.jpg&quot; alt=&quot;&quot; data-align=&quot;center&quot; width=&quot;508&quot;&gt;</summary>
    
    
    
    
    <category term="随笔" scheme="https://catchcodes.github.io/tags/%E9%9A%8F%E7%AC%94/"/>
    
    <category term="杂谈" scheme="https://catchcodes.github.io/tags/%E6%9D%82%E8%B0%88/"/>
    
  </entry>
  
</feed>
